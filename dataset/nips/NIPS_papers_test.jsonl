{"paper": "nips0", "title": "context effects in category learning: an investigation of four probabilistic models", "abstract": "categorization is a central activity of human cognition. when an individual is asked to categorize a sequence of items, context effects arise: categorization of one item influences category decisions for subsequent items. specifically, when experimental subjects are shown an exemplar of some target category, the category prototype appears to be pulled toward the exemplar, and the prototypes of all nontarget categories appear to be pushed away. these push and pull effects diminish with experience, and likely reflect long-term learning of category boundaries. we propose and evaluate four principled probabilistic (bayesian) accounts of context effects in categorization. in all four accounts, the probability of an exemplar given a category is encoded as a gaussian density in feature space, and categorization involves computing category posteriors given an exemplar. the models differ in how the uncertainty distribution of category prototypes is represented (localist or distributed), and how it is updated following each experience (using a maximum likelihood gradient ascent, or a kalman filter update). we find that the distributed maximum-likelihood model can explain the key experimental phenomena. further, the model predicts other phenomena that were confirmed via reanalysis of the experimental data. categorization is a central activity of human cognition. we continually make decisions about characteristics of objects and individuals: is the fruit ripe? does your friend seem unhappy? is your car tire flat? should this manuscript be accepted for publication in nips? when an individual is asked to categorize a sequence of items, context effects arise: categorization of one item influences category decisions for subsequent items. intuitive naturalistic scenarios in which context effects occur are easy to imagine. for example, if one lifts a medium-weight object after lifting a light-weight or heavy-weight object, the medium weight feels heavier following the light weight than following the heavy weight. although the object-contrast effect might be due to fatigue of sensory-motor systems, many context effects in categorization are purely cognitive and cannot easily be attributed to neural habituation. for example, if you are reviewing a set of conference papers, and the first three in the set are dreadful, then even a mediocre paper seems like it might be above threshold for acceptance. another example of a category boundary shift due to context is the following. suppose you move from san diego to pittsburgh and notice that your neighbors repeatedly describe muggy, somewhat overcast days as \u00f3lovely.\u00f3 eventually, your notion of what constitutes a lovely day accommodates to your new surroundings. as we describe shortly, experimental studies have shown a fundamental link between context effects in categorization and long-term learning of category boundaries. we believe that context effects can be viewed as a reflection of a trial-to-trial learning, and the cumulative effect of these trial-to-trial modulations corresponds to what we classically consider to be category learning. consequently, any compelling model of category learning should also be capable of explaining context effects. stimulus dimension abcd example figure 1: schematic depiction of sequential effects in categorization"}
{"paper": "nips1", "title": "learning from multiple sources", "abstract": "we consider the problem of learning accurate models from multiple sources of \u00f2nearby\u00f3 data. given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. this theory is applicable in a broad decision-theoretic learning framework, and yields results for classification and regression generally, and for density estimation within the exponential family. a key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest."}
{"paper": "nips2", "title": "adaptor grammars: a framework for specifying compositional nonparametric bayesian models", "abstract": "this paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (pcfgs). adaptor grammars augment the probabilistic rules of pcfgs with \u00f2adaptors\u00f3 that can induce dependencies among successive uses. with a particular choice of adaptor, based on the pitman-yor process, nonparametric bayesian models of language using dirichlet processes and hierarchical dirichlet processes can be written as simple grammars. we present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric bayesian models can be expressed within this framework."}
{"paper": "nips3", "title": "mlle: modified locally linear embedding using multiple weights", "abstract": "the locally linear embedding (lle) is improved by introducing multiple linearly independent local weight vectors for each neighborhood. we characterize the reconstruction weights and show the existence of the linearly independent weight vectors at each neighborhood. the modi ed locally linear embedding (mlle) proposed in this paper is much stable. it can retrieve the ideal embedding if mlle is applied on data points sampled from an isometric manifold. mlle is also compared with the local tangent space alignment (ltsa). numerical examples are given that show the improvement and ef ciency of mlle. keywords: manifold learning, weight vector, algorithm"}
{"paper": "nips4", "title": "sample complexity of policy search with known dynamics", "abstract": "we consider methods that try to find a good policy for a markov decision process by choosing one from a given class. the policy is chosen based on its empirical performance in simulations. we are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. we show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. previously, such results were derived by assuming boundedness of pseudodimension and lipschitz continuity. these assumptions and ours are both stronger than the usual combinatorial complexity measures. we show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufficient."}
{"paper": "nips5", "title": "a hidden markov dirichlet process model for genetic recombination in open ancestral space", "abstract": "we present a new statistical framework called hidden markov dirichlet process (hmdp) to jointly model the genetic recombinations among possibly infinite number of founders and the coalescence-with-mutation events in the resulting genealogies. the hmdp posits that a haplotype of genetic markers is generated by a sequence of recombination events that select an ancestor for each locus from an unbounded set of founders according to a 1st-order markov transition process. conjoining this process with a mutation model, our method accommodates both between-lineage recombination and within-lineage sequence variations, and leads to a compact and natural interpretation of the population structure and inheritance process underlying haplotype data. we have developed an efficient sampling algorithmforhmdpbasedonatwo-levelnestedp\u00abolyaurnscheme. onbothsimulated and real snp haplotype data, our method performs competitively or significantly better than extant methods in uncovering the recombination hotspots along chromosomalloci; andinadditionitalsoinferstheancestralgeneticpatternsandoffers a highly accurate map of ancestral compositions of modern populations."}
{"paper": "nips6", "title": "combining causal and similarity-based reasoning", "abstract": "everyday inductive reasoning draws on many kinds of knowledge, including knowledge about causal relationships between properties and knowledge about similarity relationships between objects. previous accounts of inductive reasoning generally focus on just one kind of knowledge: models of causal reasoning often focus on relationships between properties, and models of similarity-based reasoning often focus on similarity relationships between objects. we present a model of inductive reasoning that incorporates both kinds of knowledge. two experiments show that our model makes accurate quantitative predictions in a task where subjects combine knowledge about causal relationships between properties with knowledge about similarity between biological species."}
{"paper": "nips7", "title": "near-uniform sampling of combinatorial spaces using xor constraints", "abstract": "we propose a new technique for sampling the solutions of combinatorial problems in a near-uniform manner. we focus on problems speci ed as a boolean formula, i.e., on sat instances. sampling for sat problems has been shown to have interesting connections with probabilistic reasoning, making practical sampling algorithms for sat highly desirable. the best current approaches are based on markov chain monte carlo methods, which have some practical limitations. our approach exploits combinatorial properties of random parity (xor) constraints to prune away solutions near-uniformly. the nal sample is identi ed amongst the remaining ones using a state-of-the-art sat solver. the resulting sampling distribution is provably arbitrarily close to uniform. our experiments show that our technique achieves a signi cantly better sampling quality than the best alternative."}
{"paper": "nips8", "title": "temporal coding using the response properties of spiking neurons", "abstract": "in biological neurons, the timing of a spike depends on the timing of synaptic currents, in a way that is classically described by the phase response curve. this has implications for temporal coding: an action potential that arrives on a synapse has an implicit meaning, that depends on the position of the postsynaptic neuron on the firing cycle. here we show that this implicit code can be used to perform computations. using theta neurons, we derive a spike-timing dependent learning rule from an error criterion. we demonstrate how to train an auto-encoder neural network using this rule."}
{"paper": "nips9", "title": "dynamic foreground", "abstract": "in this paper, we propose a novel exemplar-based approach to extract dynamic foreground regions from a changing background within a collection of images or a video sequence. by using image segmentation as a pre-processing step, we convert this traditional pixel-wise labeling problem into a lower-dimensional supervised, binary labeling procedure on image segments. our approach consists of three steps. first, a set of random image patches are spatially and adaptively sampled within each segment. second, these sets of extracted samples are formed into two \u00f2bags of patches\u00f3 to model the foreground/background appearance, respectively. we perform a novel bidirectional consistency check between new patches from incoming frames and current \u00f2bags of patches\u00f3 to reject outliers, control model rigidity and make the model adaptive to new observations. within each bag, image patches are further partitioned and resampled to create an evolving appearance model. finally, the foreground/background decision over segments in an image is formulated using an aggregation function defined on the similarity measurements of sampled patches relative to the foreground and background models. the essence of the algorithm is conceptually simple and can be easily implemented within a few hundred lines of matlab code. we evaluate and validate the proposed approach by extensive real examples of the object-level image mapping and tracking within a variety of challenging environments. we also show that it is straightforward to apply our problem formulation on non-rigid object tracking with difficult surveillance videos."}
{"paper": "nips10", "title": "conditional mean field", "abstract": "despite all the attention paid to variational methods based on sum-product message passing (loopy belief propagation, tree-reweighted sum-product), these methods are still bound to inference on a small set of probabilistic models. mean field approximations have been applied to a broader set of problems, but the solutions are often poor. we propose a new class of conditionally-specified variational approximations based on mean field theory. while not usable on their own, combined with sequential monte carlo they produce guaranteed improvements over conventional mean field. moreover, experiments on a well-studied problem\u00f1 inferring the stable configurations of the ising spin glass\u00f1show that the solutions can be significantly better than those obtained using sum-product-based methods."}
{"paper": "nips11", "title": "chained boosting", "abstract": "we describe a method to learn to make sequential stopping decisions, such as those made along a processing pipeline. we envision a scenario in which a series of decisions must be made as to whether to continue to process. further processing costs time and resources, but may add value. our goal is to create, based on historic data, a series of decision rules (one at each stage in the pipeline) that decide, based on information gathered up to that point, whether to continue processing the part. we demonstrate how our framework encompasses problems from manufacturing to vision processing. we derive a quadratic (in the number of decisions) bound on testing performance and provide empirical results on object detection."}
{"paper": "nips12", "title": "modelling transcriptional regulation using gaussian processes", "abstract": "modelling the dynamics of transcriptional processes in the cell requires the knowledge of a number of key biological quantities. while some of them are relatively easy to measure, such as mrna decay rates and mrna abundance levels, it is still very hard to measure the active concentration levels of the transcription factor proteins that drive the process and the sensitivity of target genes to these concentrations. in this paper we show how these quantities for a given transcription factor can be inferred from gene expression levels of a set of known target genes. we treat the protein concentration as a latent function with a gaussian process prior, and include the sensitivities, mrna decay rates and baseline expression levels as hyperparameters. we apply this procedure to a human leukemia dataset, focusing on the tumour repressor p53 and obtaining results in good accordance with recent biological studies. introduction recent advances in molecular biology have brought about a revolution in our understanding of cellular processes. microarray technology now allows measurement of mrna abundance on a genome-wide scale, and techniques such as chromatin immunoprecipitation (chip) have largely unveiled the wiring of the cellular transcriptional regulatory network, identifying which genes are bound by which transcription factors. however, a full quantitative description of the regulatory mechanism of transcription requires the knowledge of a number of other biological quantities: first of all the concentration levels of active transcription factor proteins, but also a number of genespecific constants such as the baseline expression level for a gene, the rate of decay of its mrna and the sensitivity with which target genes react to a given transcription factor protein concentration. while some of these quantities can be measured (e.g. mrna decay rates), most of them are very hard to measure with current techniques, and have therefore to be inferred from the available data. this is often done following one of two complementary approaches. one can formulate a large scale simplified model of regulation (for example assuming a linear response to protein concentrations) and then combine network architecture data and gene expression data to infer transcription factors\u00f5 protein concentrations on a genome-wide scale. this line of research was started in [2] and then extended further to include gene-specific effects in [8, 9]. alternatively, one can formulate a realistic model of a small subnetwork where few transcription factors regulate a small number of established target genes, trying to include the finer points of the dynamics of transcriptional regulation. in this paper we follow the second approach, focussing on the simplest subnetwork consisting of one transcription factor regulating its target genes, but using a detailed model of the interaction dynamics to infer the transcription factor concentrations and the gene specific constants. this problem was recently studied by barenco et al. [1] and by rogers et al. [7]. in these studies, parametric models were developed describing the rate of production of certain genes as a function of the concentration of transcription factor protein at some specified time points. markov chain monte carlo (mcmc) methods were then used to carry out bayesian inference of the protein concentrations, requiring substantial computational resources and limiting the inference to the discrete time-points where the data was collected. we show here how a gaussian process model provides a simple and computationally efficient method for bayesian inference of continuous transcription factor concentration profiles and associated model parameters. gaussian processes have been used effectively in a number of machine learning and statistical applications [6]. their use in this context is novel, as far as we know, and leads to several advantages. firstly, it allows for the inference of continuous quantities (concentration profiles) without discretization, therefore accounting naturally for the temporal structure of the data. secondly, it avoids the use of cumbersome interpolation techniques to estimate mrna production rates from mrna abundance data, and it allows us to deal naturally with the noise inherent in the measurements. finally, it greatly outstrips mcmc techniques in terms of computational efficiency, which we expect to be crucial in future extensions to more complex (and realistic) regulatory networks."}
{"paper": "nips13", "title": "comparative gene prediction using conditional random fields", "abstract": "computational gene prediction using generative models has reached a plateau, with several groups converging to a generalized hidden markov model (ghmm) incorporating phylogenetic models of nucleotide sequence evolution. further improvements in gene calling accuracy are likely to come through new methods that incorporate additional data, both comparative and species specific. conditional random fields (crfs), which directly model the conditional probability p(y|x) of a vector of hidden states conditioned on a set of observations, provide a unified framework for combining probabilistic and non-probabilistic information and have been shown to outperform hmms on sequence labeling tasks in natural language processing. we describe the use of crfs for comparative gene prediction. we implement a model that encapsulates both a phylogenetic-ghmm (our baseline comparative model) and additional non-probabilistic features. we tested our model on the genome sequence of the fungal human pathogen cryptococcus neoformans. our baselinecomparativemodeldisplaysaccuracycomparabletothethebestavailable gene prediction tool for this organism. moreover, we show that discriminative training and the incorporation of non-probabilistic evidence significantly improve performance. our software implementation, conrad, is freely available with an open source license at http://www.broad.mit.edu/annotation/conrad/."}
{"paper": "nips14", "title": "fundamental limitations of spectral clustering methods", "abstract": "spectral clustering methods are common graph-based approaches to clustering of data. spectral clustering algorithms typically start from local information encoded in a weighted graph on the data and cluster according to the global eigenvectors of the corresponding (normalized) similarity matrix. one contribution of this paper is to present fundamental limitations of this general local to global approach. we show that based only on local information, the underlying normalized cut functional is not a suitable measure for the quality of clustering. further, even with a suitable similarity measure, we show that the first few eigenvectors of such adjacency matrices cannot successfully cluster datasets that contain structures at different scales of size and density. these findings provide theoretical explanations to some empirically observed characteristics of these methods. based on our theoretical analysis, a second contribution of this paper is a novel diffusion based measure to evaluate the coherence of individual clusters. our measure can be used in conjunction with any graph-based clustering method (global, top-down, bottom-up), it is scale-free and can determine coherent clusters at all scales. we present simple examples where various spectral clustering algorithms fail, whereas clustering utilizing this coherence measure finds the correct clusters at all scales. keywords: clustering, kernels, learning theory."}
{"paper": "nips15", "title": "a recipe for optimizing a time-histogram", "abstract": "the time-histogram method is a handy tool for capturing the instantaneous rate of spike occurrence. in most of the neurophysiological literature, the bin size that critically determines the goodness of the fit of the time-histogram to the underlying rate has been selected by individual researchers in an unsystematic manner. we propose an objective method for selecting the bin size of a time-histogram from the spike data, so that the time-histogram best approximates the unknown underlying rate. the resolution of the histogram increases, or the optimal bin size decreases, with the number of spike sequences sampled. it is notable that the optimal bin size diverges if only a small number of experimental trials are available from a moderately fluctuating rate process. in this case, any attempt to characterize the underlying spike rate will lead to spurious results. given a paucity of data, our method can also suggest how many more trials are needed until the set of data can be analyzed with the required resolution."}
{"paper": "nips16", "title": "large margin gaussian mixture models for automatic speech recognition", "abstract": "we study the problem of parameter estimation in gaussian mixture models (gmms), as they are used in continuous density hidden markov models for automatic speech recognition (asr). as in support vector machines, we propose a learning algorithm based on the goal of margin maximization. unlike earlier work on max-margin markov networks, our algorithm is specifically geared to the modeling of real-valued observations (such as acoustic feature vectors) using gmms. unlike previous discriminative methods for asr, such as maximum mutual information and minimum classification error, the learning problem in our framework is convex: in particular, it can be cast as a convex optimization over a parameter space of positive semidefinite matrices. the optimization can be performed efficiently with simple gradient-based methods that scale well to large problems. we obtain competitive results for phonetic recognition on the timit speech corpus."}
{"paper": "nips17", "title": "hyperparameter learning for graph based semi-supervised learning algorithms", "abstract": "semi-supervised learning algorithms have been successfully applied in many applications with scarce labeled data, by utilizing the unlabeled data. one important category is graph based semi-supervised learning algorithms, for which the performance depends considerably on the quality of the graph, or its hyperparameters. in this paper, we deal with the less explored problem of learning the graphs. we propose a graph learning method for the harmonic energy minimization method; this is done by minimizing the leave-one-out prediction error on labeled data points. we use a gradient based method and designed an efficient algorithm which significantly accelerates the calculation of the gradient by applying the matrix inversion lemma and using careful pre-computation. experimental results show that the graph learning method is effective in improving the performance of the classification algorithm."}
{"paper": "nips18", "title": "attribute-efficient learning of linear threshold functions under unconcentrated distributions", "abstract": "we consider the well-studied problem of learning decision lists using few examples when many irrelevant features are present. we show that smooth boosting algorithms such as madaboost can ef ciently learn decision lists of length k over n boolean variables using poly(k; logn) many examples provided that the marginal distribution over the relevant variables is not too concentrated in an l2-norm sense. using a recent result of h astad, we extend the analysis to obtain a similar (though quantitatively weaker) result for learning arbitrary linear threshold functions with k nonzero coef cients. experimental results indicate that the use of a smooth boosting algorithm, which plays a crucial role in our analysis, has an impact on the actual performance of the algorithm."}
{"paper": "nips19", "title": "implicit surfaces with globally regularised and compactly supported basis functions", "abstract": "we consider the problem of constructing a function whose zero set is to represent a surface, given sample points with surface normal vectors. the contributions include a novel means of regularising multi-scale compactly supported basis functions that leads to the desirable properties previously only associated with fully supported bases, and show equivalence to a gaussian process with modified covariance function. we also provide a regularisation framework for simpler and more direct treatment of surface normals, along with a corresponding generalisation of the representer theorem. we demonstrate the techniques on 3d problems of up to 14 million data points, as well as 4d time series data."}
{"paper": "nips20", "title": "multi-robot negotiation: approximating the set of subgame perfect equilibria in general sum stochastic games", "abstract": "in real-world planning problems, we must reason not only about our own goals, but about the goals of other agents with which we may interact. often these agents\u00f5 goals are neither completely aligned with our own nor directly opposed to them. instead there are opportunities for cooperation: by joining forces, the agents can all achieve higher utility than they could separately. but, in order to cooperate, the agents must negotiate a mutually acceptable plan from among the many possible ones, and each agent must trust that the others will follow their parts of the deal. research in multi-agent planning has often avoided the problem of making sure that all agents have an incentive to follow a proposed joint plan. on the other hand, while game theoretic algorithms handle incentives correctly, they often don\u00f5t scale to large planning problems. in this paper we attempt to bridge the gap between these two lines of research: we present an e\u00f0cient game-theoretic approximate planning algorithm, along with a negotiation protocol which encourages agents to compute and agree on joint plans that are fair and optimal in a sense deflned below. we demonstrate our algorithm and protocol on two simple robotic planning problems."}
{"paper": "nips21", "title": "a probabilistic algorithm integrating source localization and noise suppression for meg and eeg data", "abstract": "we have developed a novel algorithm for integrating source localization and noise suppression based on a probabilistic graphical model of stimulus-evoked meg/eeg data. our algorithm localizes multiple dipoles while suppressing noise sources with the computational complexity equivalent to a single dipole scan, and is therefore more efficient than traditional multidipole fitting procedures. in simulation, the algorithm can accurately localize and estimate the time course of several simultaneously-active dipoles, with rotating or fixed orientation, at noise levels typical for averaged meg data. furthermore, the algorithm is superior to beamforming techniques, which we show to be an approximation to our graphical model, in estimation of temporally correlated sources. success of this algorithm for localizing auditory cortex in a tumor patient and for localizing an epileptic spike source are also demonstrated."}
{"paper": "nips22", "title": "an oracle inequality for clipped regularized risk minimizers", "abstract": "we establish a general oracle inequality for clipped approximate minimizers of regularized empirical risks and apply this inequality to support vector machine (svm) type algorithms. we then show that for svms using gaussian rbf kernels for classification this oracle inequality leads to learning rates that are faster than the ones established in [9]. finally, we use our oracle inequality to show that a simple parameter selection approach based on a validation set can yield the same fast learning rates without knowing the noise exponents which were required to be known a-priori in [9]."}
{"paper": "nips23", "title": "balanced graph matching", "abstract": "graph matching is a fundamental problem in computer vision and machine learning. we present two contributions. first, we give a new spectral relaxation technique for approximate solutions to matching problems, that naturally incorporates one-to-one or one-to-many constraints within the relaxation scheme. the second is a normalization procedure for existing graph matching scoring functions that can dramatically improve the matching accuracy. it is based on a reinterpretation of the graph matching compatibility matrix as a bipartite graph on edges for which we seek a bistochastic normalization. we evaluate our two contributions on a comprehensive test set of random graph matching problems, as well as on image correspondence problem. our normalization procedure can be used to improve the performance of many existing graph matching algorithms, including spectral matching, graduated assignment and semidefinite programming."}
{"paper": "nips24", "title": "stratification learning: detecting mixed density and dimensionality in high dimensional point clouds", "abstract": "the study of point cloud data sampled from a stratification, a collection of manifolds with possible different dimensions, is pursued in this paper. we present a technique for simultaneously soft clustering and estimating the mixed dimensionality and density of such structures. the framework is based on a maximum likelihood estimation of a poisson mixture model. the presentation of the approach is completed with artificial and real examples demonstrating the importance of extending manifold learning to stratification learning."}
{"paper": "nips25", "title": "predicting spike times from subthreshold dynamics of a neuron", "abstract": "it has been established that a neuron reproduces highly precise spike response to identical fluctuating input currents. we wish to accurately predict the firing times of a given neuron for any input current. for this purpose we adopt a model that mimics the dynamics of the membrane potential, and then take a cue from its dynamics for predicting the spike occurrence for a novel input current. it is found that the prediction is significantly improved by observing the state space of the membrane potential and its time derivative(s) in advance of a possible spike, in comparison to simply thresholding an instantaneous value of the estimated potential."}
{"paper": "nips26", "title": "adaboost is consistent", "abstract": "the risk, or probability of error, of the classifier produced by the adaboost algorithm is investigated. in particular, we consider the stopping strategy to be used in adaboost to achieve universal consistency. we show that provided adaboost is stopped after n? iterations\u00f1for sample size n and ? < 1\u00f1the sequence of risks of the classifiers it produces approaches the bayes risk if bayes riskl? > 0."}
{"paper": "nips27", "title": "adaptive spatial filters with predefined region of interest for eeg based brain-computer-interfaces", "abstract": "the performance of eeg-based brain-computer-interfaces (bcis) critically depends on the extraction of features from the eeg carrying information relevant for the classi cation of different mental states. for bcis employing imaginary movements of different limbs, the method of common spatial patterns (csp) has been shown to achieve excellent classi cation results. the csp-algorithm however suffers from a lack of robustness, requiring training data without artifacts for good performance. to overcome this lack of robustness, we propose an adaptive spatial lter that replaces the training data in the csp approach by a-priori information. more speci cally, we design an adaptive spatial lter that maximizes the ratio of the variance of the electric eld originating in a prede ned region of interest (roi) and the overall variance of the measured eeg. since it is known that the component of the eeg used for discriminating imaginary movements originates in the motor cortex, we design two adaptive spatial lters with the rois centered in the hand areas of the left and right motor cortex. we then use these to classify eeg data recorded during imaginary movements of the right and left hand of three subjects, and show that the adaptive spatial lters outperform the csp-algorithm, enabling classi cation rates of up to 94.7 % without artifact rejection."}
{"paper": "nips28", "title": "an em algorithm for localizing multiple sound sources in reverberant environments", "abstract": "we present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics. the method consists of a probabilistic model of binaural multisource recordings and an expectation maximization algorithm for finding the maximum likelihood parameters of that model. these parameters include distributions over delays and assignments of time-frequency regions to sources. we evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources. our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation."}
{"paper": "nips29", "title": "data integration for classification problems employing gaussian process priors", "abstract": "a fully bayesian solution to the problem of integrating possibly heterogeneous data sets within a classification setting employing gaussian process priors is presented. approximate inference schemes employing variational and expectation propagation based methods are developed and rigorously assessed. we demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classifier combination."}
{"paper": "nips30", "title": "sparse multinomial logistic regression via bayesian l1 regularisation", "abstract": "multinomial logistic regression provides the standard penalised maximumlikelihood solution to multi-class pattern recognition problems. more recently, the development of sparse multinomial logistic regression models have found application in text processing and microarray classification, where explicit identification of the most informative features is of value. in this paper, we propose a sparse multinomial logistic regression method, in which the sparsity arises from the use of a laplace prior, but where the usual regularisation parameters are integrated out analytically. evaluation over a range of benchmark datasets reveals this approach results in similar generalisation performance to that obtained using cross-validation, but at greatly reduced computational expense."}
{"paper": "nips31", "title": "geometric entropy minimization (gem) for anomaly detection and localization", "abstract": "we introduce a novel adaptive non-parametric anomaly detection approach, called gem, that is based on the minimal covering properties of k-point entropic graphs when constructed on n training samples from a nominal probability distribution. such graphs have the property that as n ! 1 their span recovers the entropy minimizing set that supports at least \u00e4 = k=n(100)% of the mass of the lebesgue part of the distribution. when a test sample falls outside of the entropy minimizing set an anomaly can be declared at a statistical level of significance fi = 1\u00e1\u00e4. a method for implementing this non-parametric anomaly detector is proposed that approximates this minimum entropy set by the influence region of a k-point entropic graph built on the training data. by implementing an incremental leave-one-out k-nearest neighbor graph on resampled subsets of the training data gem can efficiently detect outliers at a given level of significance and compute their empirical p-values. we illustrate gem for several simulated and real data sets in high dimensional feature spaces."}
{"paper": "nips32", "title": "a scalable machine learning approach to go", "abstract": "go is an ancient board game that poses unique opportunities and challenges for ai and machine learning. here we develop a machine learning approach to go, and related board games, focusing primarily on the problem of learning a good evaluation function in a scalable way. scalability is essential at multiple levels, from the library of local tactical patterns, to the integration of patterns across the board, to the size of the board itself. the system we propose is capable of automatically learning the propensity of local patterns from a library of games. propensity and other local tactical information are fed into a recursive neural network, derived from a bayesian network architecture. the network integrates local information across the board and produces local outputs that represent local territory ownership probabilities. the aggregation of these probabilities provides an effective strategic evaluation function that is an estimate of the expected area at the end (or at other stages) of the game. local area targets for training can be derived from datasets of human games. a system trained using only 9?9 amateur game data performs surprisingly well on a test set derived from 19?19 professional game data. possible directions for further improvements are briefly discussed."}
{"paper": "nips33", "title": "inferring network structure from co-occurrences", "abstract": "we consider the problem of inferring the structure of a network from cooccurrence data: observations that indicate which nodes occur in a signaling pathway but do not directly reveal node order within the pathway. this problem is motivated by network inference problems arising in computational biology and communication systems, in which it is difficult or impossible to obtain precise time ordering information. without order information, every permutation of the activated nodes leads to a different feasible solution, resulting in combinatorial explosion of the feasible set. however, physical principles underlying most networked systems suggest that not all feasible solutions are equally likely. intuitively, nodes that co-occur more frequently are probably more closely connected. building on this intuition, we model path co-occurrences as randomly shuffled samples of a random walk on the network. we derive a computationally efficient network inference algorithm and, via novel concentration inequalities for importance sampling estimators, prove that a polynomial complexity monte carlo version of the algorithm converges with high probability."}
{"paper": "nips34", "title": "accelerated variational dirichlet process mixtures", "abstract": "dirichlet process (dp) mixture models are promising candidates for clustering applications where the number of clusters is unknown a priori. due to computational considerations these models are unfortunately unsuitable for large scale data-mining applications. we propose a class of deterministic accelerated dp mixture models that can routinely handle millions of data-cases. the speedup is achieved by incorporating kd-trees into a variational bayesian algorithm for dp mixtures in the stick-breaking representation, similar to that of blei and jordan (2005). our algorithm differs in the use of kd-trees and in the way we handle truncation: we only assume that the variational distributions are xed at their priors after a certain level. experiments show that speedups relative to the standard variational algorithm can be signi cant."}
{"paper": "nips35", "title": "multi-task feature learning", "abstract": "we present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. the method builds upon the wellknown 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. we show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it. the algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the latter step we learn commonacross-tasks representations and in the former step we learn task-specific functions using these representations. we report experiments on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently. our algorithm can also be used, as a special case, to simply select \u00f0 not learn \u00f0 a few common features across the tasks."}
{"paper": "nips36", "title": "doubly stochastic normalization for spectral clustering", "abstract": "in this paper we focus on the issue of normalization of the affinity matrix in spectral clustering. we show that the difference between n-cuts and ratio-cuts is in the error measure being used (relative-entropy versus l1 norm) in finding the closest doubly-stochastic matrix to the input affinity matrix. we then develop a scheme for finding the optimal, under frobenius norm, doubly-stochastic approximation using von-neumann\u00f5s successive projections lemma. the new normalization scheme is simple and efficient and provides superior clustering performance over many of the standardized tests."}
{"paper": "nips37", "title": "on transductive regression", "abstract": "in many modern large-scale learning applications, the amount of unlabeled data far exceeds that of labeled data. a common instance of this problem is the transductive setting where the unlabeled test points are known to the learning algorithm. this paper presents a study of regression problems in that setting. it presents explicit vc-dimension error bounds for transductive regression that hold for all bounded loss functions and coincide with the tight classification bounds of vapnik when applied to classification. it also presents a new transductive regression algorithm inspired by our bound that admits a primal and kernelized closedform solution and deals efficiently with large amounts of unlabeled data. the algorithm exploits the position of unlabeled points to locally estimate their labels and then uses a global optimization to ensure robust predictions. our study also includes the results of experiments with several publicly available regression data sets with up to 20,000 unlabeled examples. the comparison with other transductive regression algorithms shows that it performs well and that it can scale to large data sets."}
{"paper": "nips38", "title": "real-time adaptive information-theoretic optimization of neurophysiological experiments", "abstract": "adaptively optimizing experiments can significantly reduce the number of trials needed to characterize neural responses using parametric statistical models. however, the potential for these methods has been limited to date by severe computational challenges: choosing the stimulus which will provide the most informationaboutthe(typicallyhigh-dimensional)modelparametersrequiresevaluatinga high-dimensional integration and optimization in near-real time. here we present a fast algorithm for choosing the optimal (most informative) stimulus based on a fisherapproximationoftheshannoninformationandspecializednumericallinear algebra techniques. this algorithm requires only low-rank matrix manipulations and a one-dimensional linesearch to choose the stimulus and is therefore efficient even for high-dimensional stimulus and parameter spaces; for example, we require just 15 milliseconds on a desktop computer to optimize a 100-dimensional stimulus. our algorithm therefore makes real-time adaptive experimental design feasible. simulation results show that model parameters can be estimated much more efficiently using these adaptive techniques than by using random (nonadaptive) stimuli. finally, we generalize the algorithm to efficiently handle both fast adaptationduetospike-historyeffectsandslow,non-systematicdriftsinthemodel parameters. maximizing the efficiency of data collection is important in any experimental setting. in neurophysiological experiments, minimizing the number of trials needed to characterize a neural system is essential for maintaining the viability of a preparation and ensuring robust results. as a result, various approaches have been developed to optimize neurophysiology experiments online in order to choose the \u00f2best\u00f3 stimuli given prior knowledge of the system and the observed history of the cell\u00f5s responses. the \u00f2best\u00f3 stimulus can be defined a number of different ways depending on the experimental objectives. one reasonable choice, if we are interested in finding a neuron\u00f5s \u00f2preferred stimulus,\u00f3 is the stimulus which maximizes the firing rate of the neuron [1, 2, 3, 4]. alternatively, when investigating the coding properties of sensory cells it makes sense to define the optimal stimulus in terms of the mutual information between the stimulus and response [5]. here we take a system identification approach: we define the optimal stimulus as the one which tells us the most about how a neural system responds to its inputs [6, 7]. we consider neural systems in ?http://www.prism.gatech.edu/?gtg120z http://www.stat.columbia.edu/?liam which the probability p(rt|{vectorxt,vectorxt?1,...,vectorxt?tk},{rt?1,...,rt?ta}) of the neural response rt given the current and past stimuli {vectorxt,vectorxt?1,...,vectorxt?tk}, and the observed recent history of the neuron\u00f5s activity, {rt?1,...,rt?ta}, can be described by a model p(rt|{vectorxt},{rt?1},vector?), specified by a finite vector of parameters vector?. since we estimate these parameters from experimental trials, we want to choose our stimuli so as to minimize the number of trials needed to robustly estimate vector?. two inconvenient facts make it difficult to realize this goal in a computationally efficient manner:"}
{"paper": "nips39", "title": "analysis of contour motions", "abstract": "a reliable motion estimation algorithm must function under a wide range of conditions. one regime, which we consider here, is the case of moving objects with contours but no visible texture. tracking distinctive features such as corners can disambiguate the motion of contours, but spurious features such as t-junctions can be badly misleading. it is dif cult to determine the reliability of motion from local measurements, since a full rank covariance matrix can result from both real and spurious features. we propose a novel approach that avoids these points altogether, and derives global motion estimates by utilizing information from three levels of contour analysis: edgelets, boundary fragments and contours. boundary fragment are chains of orientated edgelets, for which we derive motion estimates from local evidence. the uncertainties of the local estimates are disambiguated after the boundary fragments are properly grouped into contours. the grouping is done by constructing a graphical model and marginalizing it using rejection sampling. we propose two equivalent representations in this graphical model, reversible switch variables attached to the ends of fragments and fragment chains, to capture both local and global statistics of boundaries. our system is successfully applied to both synthetic and real video sequences containing high-contrast boundaries and textureless regions. the system produces good motion estimates along with properly grouped and completed contours."}
{"paper": "nips40", "title": "implicit online learning with kernels", "abstract": "we present two new algorithms for online learning in reproducing kernel hilbert spaces. our first algorithm, ilk (implicit online learning with kernels), employs a new, implicit update technique that can be applied to a wide variety of convex loss functions. we then introduce a bounded memory version, silk (sparse ilk), that maintains a compact representation of the predictor without compromising solution quality, even in non-stationary environments. we prove loss bounds and analyze the convergence rate of both. experimental evidence shows that our proposed algorithms outperform current methods on synthetic and real data."}
{"paper": "nips41", "title": "generalized maximum margin clustering and unsupervised kernel learning", "abstract": "maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. it extends the theory of support vector machine to unsupervised learning. despite its good performance, there are three major problems with maximum margin clustering that question its e\u00f0ciency for real-world applications. first, it is computationally expensive and di\u00f0cult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. in this paper, we propose \\generalized maximum margin clustering\" framework that addresses the above three problems simultaneously. the new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. it signiflcantly improves the computational e\u00f0ciency by reducing the number of parameters. furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. finally, we show a formal connection between maximum margin clustering and spectral clustering. we demonstrate the e\u00f0ciency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the uci repository."}
{"paper": "nips42", "title": "temporal and cross-subject probabilistic models for fmri prediction task", "abstract": "we present a probabilistic model applied to the fmri video rating prediction task of the pittsburgh brain activity interpretation competition (pbaic) [2]. our goal is to predict a time series of subjective, semantic ratings of a movie given functional mri data acquired during viewing by three subjects. our method uses conditionally trained gaussian markov random fields, which model both the relationships between the subjects\u00f5 fmri voxel measurements and the ratings, as well as the dependencies of the ratings across time steps and between subjects. we also employed non-traditional methods for feature selection and regularization that exploit the spatial structure of voxel activity in the brain. the model displayed good performance in predicting the scored ratings for the three subjects in test data sets, and a variant of this model was the third place entrant to the 2006 pbaic."}
{"paper": "nips43", "title": "a bayesian approach to diffusion models of decision-making and response time", "abstract": "we present a computational bayesian approach for wiener diffusion models, which are prominent accounts of response time distributions in decision-making. we first develop a general closed-form analytic approximation to the response time distributions for one-dimensional diffusion processes, and derive the required wiener diffusion as a special case. we use this result to undertake bayesian modeling of benchmark data, using posterior sampling to draw inferences about the interesting psychological parameters. with the aid of the benchmark data, we show the bayesian account has several advantages, including dealing naturally with the parameter variation needed to account for some key features of the data, and providing quantitative measures to guide decisions about model construction."}
{"paper": "nips44", "title": "learning dense 3d correspondence", "abstract": "establishing correspondence between distinct objects is an important and nontrivial task: correctness of the correspondence hinges on properties which are difficult to capture in an a priori criterion. while previous work has used a priori criteria which in some cases led to very good results, the present paper explores whether it is possible to learn a combination of features that, for a given training set of aligned human heads, characterizes the notion of correct correspondence. by optimizing this criterion, we are then able to compute correspondence and morphs for novel heads."}
{"paper": "nips45", "title": "modeling general and specific aspects of documents with a probabilistic topic model", "abstract": "techniques such as probabilistic topic models and latent-semantic indexing have been shown to be broadly useful at automatically extracting the topical or semantic content of documents, or more generally for dimension-reduction of sparse count data. these types of models and algorithms can be viewed as generating an abstraction from the words in a document to a lower-dimensional latent variable representation that captures what the document is generally about beyond the specific words it contains. in this paper we propose a new probabilistic model that tempers this approach by representing each document as a combination of (a) a background distribution over common words, (b) a mixture distribution over general topics, and (c) a distribution over words that are treated as being specific to that document. we illustrate how this model can be used for information retrieval by matching documents both at a general topic level and at a specific word level, providing an advantage over techniques that only match documents at a general level (such as topic models or latent-sematic indexing) or that only match documents at the specific word level (such as tf-idf)."}
{"paper": "nips46", "title": "blind motion deblurring using image statistics", "abstract": "we address the problem of blind motion deblurring from a single image, caused by a few moving objects. in such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. however, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. our approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur. assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box filter blurs. this enables us to model the expected derivatives distributions as a function of the width of the blur kernel. those distributions are surprisingly powerful in discriminating regions with different blurs. the approach produces convincing deconvolution results on real world images with rich texture."}
{"paper": "nips47", "title": "active learning for misspecified generalized linear models", "abstract": "active learning refers to algorithmic frameworks aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a learning method. in this paper, we present an asymptotic analysis of active learning for generalized linear models. our analysis holds under the common practical situation of model misspecification, and is based on realistic assumptions regarding the nature of the sampling distribution, which is usually neither independent nor identically distributed. we derive unbiased estimators of generalization performance, as well as estimators of expected reduction in generalization error after adding a new training data point, that allow us to optimize its sampling distribution through a convex optimization problem. our analysis naturally leads to an algorithm for sequential active learning which is applicable for all tasks supported by generalized linear models (e.g., binary classification, multi-class classification, regression) and can be applied in non-linear settings through the use of mercer kernels."}
{"paper": "nips48", "title": "a small world threshold for economic network formation", "abstract": "we introduce a game-theoretic model for network formation inspired by earlier stochastic models that mix localized and long-distance connectivity. in this model, players may purchase edges at distance d at a cost of d , and wish to minimize the sum of their edge purchases and their average distance to other players. in this model, we show there is a striking small world threshold phenomenon: in two dimensions, if < 2 then every nash equilibrium results in a network of constant diameter (independent of network size), and if > 2 then every nash equilibrium results in a network whose diameter grows as a root of the network size, and thus is unbounded. we contrast our results with those of kleinberg [8] in a stochastic model, and empirically investigate the navigability of equilibrium networks. our theoretical results all generalize to higher dimensions."}
{"paper": "nips49", "title": "mixture regression for covariate shift", "abstract": "in supervised learning there is a typical presumption that the training and test points are taken from the same distribution. in practice this assumption is commonly violated. the situations where the training and test data are from different distributions is called covariate shift. recent work has examined techniques for dealing with covariate shift in terms of minimisation of generalisation error. as yet the literature lacks a bayesian generative perspective on this problem. this paper tackles this issue for regression models. recent work on covariate shift can be understood in terms of mixture regression. using this view, we obtain a general approach to regression under covariate shift, which reproduces previous work as a special case. the main advantages of this new formulation over previous models for covariate shift are that we no longer need to presume the test and training densities are known, the regression and density estimation are combined into a single procedure, and previous methods are reproduced as special cases of this procedure, shedding light on the implicit assumptions the methods are making."}
{"paper": "nips50", "title": "emergence of conjunctive visual features by quadratic independent component analysis", "abstract": "in previous studies, quadratic modelling of natural images has resulted in cell models that react strongly to edges and bars. here we apply quadratic independent component analysis to natural image patches, and show that up to a small approximation error, the estimated components are computing conjunctions of two linear features. these conjunctive features appear to represent not only edges and bars, but also inherently two-dimensional stimuli, such as corners. in addition, we show that for many of the components, the underlying linear features have essentially v1 simple cell receptive field characteristics. our results indicate that the development of the v2 cells preferring angles and corners may be partly explainable by the principle of unsupervised sparse coding of natural images."}
{"paper": "nips51", "title": "multi-instance multi-label learning with application to scene classification", "abstract": "in this paper, we formalize multi-instance multi-label learning, where each training example is associated with not only multiple instances but also multiple class labels. such a problem can occur in many real-world tasks, e.g. an image usually contains multiple regions each of which can be described by a feature vector, and the image can belong to multiple categories since its semantics can be recognized in different ways. we analyze the relationship between multi-instance multi-label learning and the learning frameworks of traditional supervised learning, multiinstance learning and multi-label learning. then, we propose the mimlboost and mimlsvm algorithms which achieve good performance in an application to scene classification."}
{"paper": "nips52", "title": "branch and bound for semi-supervised support vector machines", "abstract": "semi-supervised svms (s3vm) attempt to learn low-density separators by maximizing the margin over labeled and unlabeled examples. the associated optimization problem is non-convex. to examine the full potential of s3vms modulo local minima problems in current implementations, we apply branch and bound techniques for obtaining exact, globally optimal solutions. empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situations where other implementations fail completely. while our current implementation is only applicable to small datasets, we discuss variants that can potentially lead to practically useful algorithms."}
{"paper": "nips53", "title": "a kernel subspace method by stochastic realization for learning nonlinear dynamical systems", "abstract": "in this paper, we present a subspace method for learning nonlinear dynamical systems based on stochastic realization, in which state vectors are chosen using kernel canonical correlation analysis, and then state-space systems are identified through regression with the state vectors. we construct the theoretical underpinning and derive a concrete algorithm for nonlinear identification. the obtained algorithm needs no iterative optimization procedure and can be implemented on the basis of fast and reliable numerical schemes. the simulation result shows that our algorithm can express dynamics with a high degree of accuracy."}
{"paper": "nips54", "title": "tighter pac-bayes bounds", "abstract": "this paper proposes a pac-bayes bound to measure the performance of support vector machine (svm) classifiers. the bound is based on learning a prior over the distribution of classifiers with a part of the training samples. experimental work shows that this bound is tighter than the original pac-bayes, resulting in an enhancement of the predictive capabilities of the pac-bayes bound. in addition, it is shown that the use of this bound as a means to estimate the hyperparameters of the classifier compares favourably with cross validation in terms of accuracy of the model, while saving a lot of computational burden."}
{"paper": "nips55", "title": "an efficient method for gradient-based adaptation of hyperparameters in svm models", "abstract": "we consider the task of tuning hyperparameters in svm models based on minimizing a smooth performance validation function, e.g., smoothed k-fold crossvalidation error, using non-linear optimization techniques. the key computation in this approach is that of the gradient of the validation function with respect to hyperparameters. we show that for large-scale problems involving a wide choice of kernel-based models and validation functions, this computation can be very ef- ciently done; often within just a fraction of the training time. empirical results show that a near-optimal set of hyperparameters can be identi ed by our approach with very few training rounds and gradient computations. ."}
{"paper": "nips56", "title": "optimal single-class classification strategies", "abstract": "we consider single-class classification (scc) as a two-person game between the learner and an adversary. in this game the target distribution is completely known to the learner and the learner\u00f5s goal is to construct a classifier capable of guaranteeing a given tolerance for the false-positive error while minimizing the false negative error. we identify both \u00f2hard\u00f3 and \u00f2soft\u00f3 optimal classification strategies for different types of games and demonstrate that soft classification can provide a significant advantage. our optimal strategies and bounds provide worst-case lower bounds for standard, finite-sample scc and also motivate new approaches to solving scc."}
{"paper": "nips57", "title": "isotonic conditional random fields and local sentiment flow", "abstract": "we examine the problem of predicting local sentiment flow in documents, and its application to several areas of text analysis. formally, the problem is stated as predicting an ordinal sequence based on a sequence of word sets. in the spirit of isotonic regression, we develop a variant of conditional random fields that is wellsuited to handle this problem. using the m\u00acobius transform, we express the model as a simple convex optimization problem. experiments demonstrate the model and its applications to sentiment prediction, style analysis, and text summarization."}
{"paper": "nips58", "title": "subordinate class recognition using relational object models", "abstract": "we address the problem of sub-ordinate class recognition, like the distinction between different types of motorcycles. our approach is motivated by observations from cognitive psychology, which identify parts as the defining component of basic level categories (like motorcycles), while sub-ordinate categories are more often defined by part properties (like \u00f5jagged wheels\u00f5). accordingly, we suggest a two-stage algorithm: first, a relational part based object model is learnt using unsegmented object images from the inclusive class (e.g., motorcycles in general). the model is then used to build a class-specific vector representation for images, where each entry corresponds to a model\u00f5s part. in the second stage we train a standard discriminative classifier to classify subclass instances (e.g., cross motorcycles) based on the class-specific vector representation. we describe extensive experimental results with several subclasses. the proposed algorithm typically gives better results than a competing one-step algorithm, or a two stage algorithm where classification is based on a model of the sub-ordinate class."}
{"paper": "nips59", "title": "denoising and dimension reduction in feature space", "abstract": "we show that the relevant information about a classification problem in feature space is contained up to negligible error in a finite number of leading kernel pca components if the kernel matches the underlying learning problem. thus, kernels not only transform data sets such that good generalization can be achieved even by linear discriminant functions, but this transformation is also performed in a manner which makes economic use of feature space dimensions. in the best case, kernels provide efficient implicit representations of the data to perform classification. practically, we propose an algorithm which enables us to recover the subspace and dimensionality relevant for good classification. our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to help in model selection, and to (3) de-noise in feature space in order to yield better classification results."}
{"paper": "nips60", "title": "blind source separation for over-determined delayed mixtures", "abstract": "blind source separation, i.e. the extraction of unknown sources from a set of given signals, is relevant for many applications. a special case of this problem is dimension reduction, where the goal is to approximate a given set of signals by superpositions of a minimal number of sources. since in this case signals outnumber the sources the problem is over-determined. most popular approaches for addressing this problem are based on purely linear mixing models. however, many applications like the modeling of acoustic signals, emg signals, or movement trajectories, require temporal shift-invariance of the extracted components. this case has only rarely been treated in the computational literature, and specifically for the case of dimension reduction almost no algorithms have been proposed. we present a new algorithm for the solution of this problem, which is based on a time-frequency transformation (wigner-ville distribution) of the generative model. we show that this algorithm outperforms classical source separation algorithms for linear mixtures, and also a related method for mixtures with delays. in addition, applying the new algorithm to trajectories of human gaits, we demonstrate that it is suitable for the extraction of spatio-temporal components that are easier to interpret than components extracted with other classical algorithms."}
{"paper": "nips61", "title": "a local learning approach for clustering", "abstract": "we present a local learning approach for clustering. the basic idea is that a good clustering result should have the property that the cluster label of each data point can be well predicted based on its neighboring data and their cluster labels, using current supervised learning methods. an optimization problem is formulated such that its solution has the above property. relaxation and eigen-decomposition are applied to solve this optimization problem. we also briefly investigate the parameter selection issue and provide a simple parameter selection method for the proposed algorithm. experimental results are provided to validate the effectiveness of the proposed approach."}
{"paper": "nips62", "title": "attentional processing on a spike-based vlsi neural network", "abstract": "the neurons of the neocortex communicate by asynchronous events called action potentials (or \u00f5spikes\u00f5). however, for simplicity of simulation, most models of processing by cortical neural networks have assumed that the activations of their neurons can be approximated by event rates rather than taking account of individual spikes. the obstacle to exploring the more detailed spike processing of these networks has been reduced considerably in recent years by the development of hybrid analog-digital very-large scale integrated (hvlsi) neural networks composed of spiking neurons that are able to operate in real-time. in this paper we describe such a hvlsi neural network that performs an interesting task of selective attentional processing that was previously described for a simulated \u00f5pointer-map\u00f5 rate model by hahnloser and colleagues. we found that most of the computational features of their rate model can be reproduced in the spiking implementation; but, that spike-based processing requires a modification of the original network architecture in order to memorize a previously attended target."}
{"paper": "nips63", "title": "stochastic relational models for discriminative link prediction", "abstract": "we introduce a gaussian process (gp) framework, stochastic relational models (srm), for learning social, physical, and other relational phenomena where interactions between entities are observed. the key idea is to model the stochastic structure of entity relationships (i.e., links) via an interplay of multiple gps, each defined on one type of entities. the framework offers a discriminative approach to link prediction, namely, predicting the existences, strengths, or types of relationships between entities based on the observed linkage network as well as the attributes of entities (if given). we discuss properties and variants of srm and derive an efficient learning algorithm. very encouraging experimental results are achieved on a toy problem and a user-movie preference link prediction task. in the end we discuss the extension of srm to general relational learning tasks."}
{"paper": "nips64", "title": "handling advertisements of unknown quality in search advertising", "abstract": "we consider how a search engine should select advertisements to display with search results, in order to maximize its revenue. under the standard \u00f2pay-per-click\u00f3 arrangement, revenue depends on how well the displayed advertisements appeal to users. the main difficulty stems from new advertisements whose degree of appeal has yet to be determined. often the only reliable way of determining appeal is exploration via display to users, which detracts from exploitation of other advertisements known to have high appeal. budget constraints and finite advertisement lifetimes make it necessary to explore as well as exploit. in this paper we study the tradeoff between exploration and exploitation, modeling advertisement placement as a multi-armed bandit problem. we extend traditional bandit formulations to account for budget constraints that occur in search engine advertising markets, and derive theoretical bounds on the performance of a family of algorithms. we measure empirical performance via extensive experiments over real-world data."}
{"paper": "nips65", "title": "learning motion style synthesis from perceptual observations", "abstract": "this paper presents an algorithm for synthesis of human motion in specified styles. we use a theory of movement observation (laban movement analysis) to describe movement styles as points in a multi-dimensional perceptual space. we cast the task of learning to synthesize desired movement styles as a regression problem: sequences generated via space-time interpolation of motion capture data are used to learn a nonlinear mapping between animation parameters and movement styles in perceptual space. we demonstrate that the learned model can apply a variety of motion styles to pre-recorded motion sequences and it can extrapolate styles not originally included in the training data."}
{"paper": "nips66", "title": "the neurodynamics of belief propagation on binary markov random fields", "abstract": "we rigorously establish a close relationship between message passing algorithms and models of neurodynamics by showing that the equations of a continuous hop- eld network can be derived from the equations of belief propagation on a binary markov random eld. as hop eld networks are equipped with a lyapunov function, convergence is guaranteed. as a consequence, in the limit of many weak connections per neuron, hop eld networks exactly implement a continuous-time variant of belief propagation starting from message initialisations that prevent from running into convergence problems. our results lead to a better understanding of the role of message passing algorithms in real biological neural networks."}
{"paper": "nips67", "title": "a switched gaussian process for estimating disparity and segmentation in binocular stereo", "abstract": "this paper describes a gaussian process framework for inferring pixel-wise disparity and bi-layer segmentation of a scene given a stereo pair of images. the gaussian process covariance is parameterized by a foreground-backgroundocclusion segmentation label to model both smooth regions and discontinuities. as such, we call our model a switched gaussian process. we propose a greedy incremental algorithm for adding observations from the data and assigning segmentation labels. two observation schedules are proposed: the first treats scanlines as independent, the second uses an active learning criterion to select a sparse subset of points to measure. we show that this probabilistic framework has comparable performance to the state-of-the-art."}
{"paper": "nips68", "title": "generalized regularized least-squares learning with predefined features in a hilbert space", "abstract": "kernel-based regularized learning seeks a model in a hypothesis space by minimizing the empirical error and the model\u00f5s complexity. based on the representer theorem, the solution consists of a linear combination of translates of a kernel. this paper investigates a generalized form of representer theorem for kernel-based learning. after mapping prede ned features and translates of a kernel simultaneously onto a hypothesis space by a speci c way of constructing kernels, we proposed a new algorithm by utilizing a generalized regularizer which leaves part of the space unregularized. using a squared-loss function in calculating the empirical error, a simple convex solution is obtained which combines prede ned features with translates of the kernel. empirical evaluations have con rmed the effectiveness of the algorithm for supervised learning tasks."}
{"paper": "nips69", "title": "logarithmic online regret bounds for undiscounted reinforcement learning", "abstract": "we present a learning algorithm for undiscounted reinforcement learning. our interest lies in bounds for the algorithm\u00f5s online performance after some finite number of steps. in the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our ucrl algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy."}
{"paper": "nips70", "title": "dirichlet-enhanced spam filtering based on biased samples", "abstract": "we study a setting that is motivated by the problem of filtering spam messages for many users. each user receives messages according to an individual, unknown distribution, reflected only in the unlabeled inbox. the spam filter for a user is required to perform well with respect to this distribution. labeled messages from publicly available sources can be utilized, but they are governed by a distinct distribution, not adequately representing most inboxes. we devise a method that minimizes a loss function with respect to a user\u00f5s personal distribution based on the available biased sample. a nonparametric hierarchical bayesian model furthermore generalizes across users by learning a common prior which is imposed on new email accounts. empirically, we observe that bias-corrected learning outperforms naive reliance on the assumption of independent and identically distributed data; dirichlet-enhanced generalization across users outperforms a single (\u00f2one size fits all\u00f3) filter as well as independent filters for all users."}
{"paper": "nips71", "title": "a nonparametric approach to bottom-up visual saliency", "abstract": "this paper addresses the bottom-up influence of local image information on human eye movements. most existing computational models use a set of biologically plausible linear filters, e.g., gabor or difference-of-gaussians filters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. unfortunately, this requires many design parameters such as the number, type, and size of the front-end filters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justified. as a result, these parameters have to be chosen in a more or less ad hoc way. here, we propose to learn a visual saliency model directly from human eye movement data. the model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the field that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. experimental results show that\u00f1despite the lack of any biological prior knowledge\u00f1our model performs comparably to existing approaches, and in fact learns image features that resemble findings from several previous studies. in particular, its maximally excitatory stimuli have center-surround structure, similar to receptive fields in the early human visual system."}
{"paper": "nips72", "title": "approximate correspondences in high dimensions", "abstract": "pyramid intersection is an ef cient method for computing an approximate partial matching between two sets of feature vectors. we introduce a novel pyramid embedding based on a hierarchy of non-uniformly shaped bins that takes advantage of the underlying structure of the feature space and remains accurate even for sets with high-dimensional feature vectors. the matching similarity is computed in linear time and forms a mercer kernel. whereas previous matching approximation algorithms suffer from distortion factors that increase linearly with the feature dimension, we demonstrate that our approach can maintain constant accuracy even as the feature dimension increases. when used as a kernel in a discriminative classi er, our approach achieves improved object recognition results over a state-of-the-art set kernel."}
{"paper": "nips73", "title": "a pac-bayes risk bound for general loss functions", "abstract": "we provide a pac-bayesian bound for the expected loss of convex combinations of classifiers under a wide class of loss functions (which includes the exponential loss and the logistic loss). our numerical experiments with adaboost indicate that the proposed upper bound, computed on the training set, behaves very similarly as the true loss estimated on the testing set."}
{"paper": "nips74", "title": "unified inference for variational bayesian linear gaussian state-space models", "abstract": "linear gaussian state-space models are widely used and a bayesian treatment of parameters is therefore of considerable interest. the approximate variational bayesian method applied to these models is an attractive approach, used successfully in applications ranging from acoustics to bioinformatics. the most challenging aspect of implementing the method is in performing inference on the hidden state sequence of the model. we show how to convert the inference problem so that standard and stable kalman filtering/smoothing recursions from the literature may be applied. this is in contrast to previously published approaches based on belief propagation. our framework both simplifies and unifies the inference problem, so that future applications may be easily developed. we demonstrate the elegance of the approach on bayesian temporal ica, with an application to finding independent components in noisy eeg signals."}
{"paper": "nips75", "title": "similarity by composition", "abstract": "we propose a new approach for measuring similarity between two signals, which is applicable to many machine learning tasks, and to many signal types. we say that a signal s1 is \u00f2similar\u00f3 to a signal s2 if it is \u00f2easy\u00f3 to compose s1 from few large contiguous chunks of s2. obviously, if we use small enough pieces, then any signal can be composed of any other. therefore, the larger those pieces are, the more similar s1 is to s2. this induces a local similarity score at every point in the signal, based on the size of its supported surrounding region. these local scores can in turn be accumulated in a principled information-theoretic way into a global similarity score of the entire s1 to s2. \u00f2similarity by composition\u00f3 can be applied between pairs of signals, between groups of signals, and also between different portions of the same signal. it can therefore be employed in a wide variety of machine learning problems (clustering, classification, retrieval, segmentation, attention, saliency, labelling, etc.), and can be applied to a wide range of signal types (images, video, audio, biological data, etc.) we show a few such examples."}
{"paper": "nips76", "title": "particle filtering for nonparametric bayesian matrix factorization", "abstract": "many unsupervised learning problems can be expressed as a form of matrix factorization, reconstructing an observed data matrix as the product of two matrices of latent variables. a standard challenge in solving these problems is determining the dimensionality of the latent matrices. nonparametric bayesian matrix factorization is one way of dealing with this challenge, yielding a posterior distribution over possible factorizations of unbounded dimensionality. a drawback to this approach is that posterior estimation is typically done using gibbs sampling, which can be slow for large problems and when conjugate priors cannot be used. as an alternative, we present a particle filter for posterior estimation in nonparametric bayesian matrix factorization models. we illustrate this approach with two matrix factorization models and show favorable performance relative to gibbs sampling."}
{"paper": "nips77", "title": "bayesian image super-resolution, continued", "abstract": "this paper develops a multi-frame image super-resolution approach from a bayesian view-point by marginalizing over the unknown registration parameters relating the set of input low-resolution views. in tipping and bishop\u00f5s bayesian image super-resolution approach [16], the marginalization was over the superresolution image, necessitating the use of an unfavorable image prior. by integrating over the registration parameters rather than the high-resolution image, our method allows for more realistic prior distributions, and also reduces the dimension of the integral considerably, removing the main computational bottleneck of the other algorithm. in addition to the motion model used by tipping and bishop, illumination components are introduced into the generative model, allowing us to handle changes in lighting as well as motion. we show results on real and synthetic datasets to illustrate the ef cacy of this approach."}
{"paper": "nips78", "title": "bayesian ensemble learning", "abstract": "we develop a bayesian sum-of-trees model where each tree is constrained by a prior to be a weak learner. fitting and inference are accomplished via an iterative back tting mcmc algorithm. this model is motivated by ensemble methods in general, and boosting algorithms in particular. like boosting, each weak learner (i.e., each weak tree) contributes a small amount to the overall model. however, our procedure is de ned by a statistical model: a prior and a likelihood, while boosting is de ned by an algorithm. this model-based approach enables a full and accurate assessment of uncertainty in model predictions, while remaining highly competitive in terms of predictive accuracy."}
{"paper": "nips79", "title": "a collapsed variational bayesian inference algorithm for latent dirichlet allocation", "abstract": "latent dirichlet allocation (lda) is a bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. due to the large scale nature of these applications, current inference procedures like variational bayes and gibb sampling have been found lacking. in this paper we propose the collapsed variational bayesian inference algorithm for lda, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational bayesian inference for lda."}
{"paper": "nips80", "title": "speakers optimize information density through syntactic reduction", "abstract": "if language users are rational, they might choose to structure their utterances so as to optimize communicative properties. in particular, information-theoretic and psycholinguistic considerations suggest that this may include maximizing the uniformity of information density in an utterance. we investigate this possibility in the context of syntactic reduction, where the speaker has the option of either marking a higher-order unit (a phrase) with an extra word, or leaving it unmarked. we demonstrate that speakers are more likely to reduce less information-dense phrases. in a second step, we combine a stochastic model of structured utterance production with a logistic-regression model of syntactic reduction to study which types of cues speakers employ when estimating the predictability of upcoming elements. we demonstrate that the trend toward predictability-sensitive syntactic reduction (jaeger, 2006) is robust in the face of a wide variety of control variables, and present evidence that speakers use both surface and structural cues for predictability estimation."}
{"paper": "nips81", "title": "information bottleneck for non co-occurrence data", "abstract": "we present a general model-independent approach to the analysis of data in cases when these data do not appear in the form of co-occurrence of two variables x; y , but rather as a sample of values of an unknown (stochastic) function z(x; y ). for example, in gene expression data, the expression level z is a function of gene x and condition y ; or in movie ratings data the rating z is a function of viewer x and movie y . the approach represents a consistent extension of the information bottleneck method that has previously relied on the availability of co-occurrence statistics. by altering the relevance variable we eliminate the need in the sample of joint distribution of all input variables. this new formulation also enables simple mdl-like model complexity control and prediction of missing values of z. the approach is analyzed and shown to be on a par with the best known clustering algorithms for a wide range of domains. for the prediction of missing values (collaborative ltering) it improves the currently best known results."}
{"paper": "nips82", "title": "manifold denoising", "abstract": "we consider the problem of denoising a noisily sampled submanifold m in rd, where the submanifold m is a priori unknown and we are only given a noisy point sample. the presented denoising algorithm is based on a graph-based diffusion process of the point sample. we analyze this diffusion process using recent results about the convergence of graph laplacians. in the experiments we show that our method is capable of dealing with non-trivial high-dimensional noise. moreover using the denoising algorithm as pre-processing method we can improve the results of a semi-supervised learning algorithm."}
{"paper": "nips83", "title": "an approach to bounded rationality", "abstract": "a central question in game theory, learning, and other fields is how a rational intelligent agent should behave in a complex environment, given that it cannot perform unbounded computations. we study strategic aspects of this question by formulating a simple model of a game with additional costs (computational or otherwise) for each strategy. while a zero-sum game with strategy costs is no longer zerosum, we show that its nash equilibria have an interesting structure and the game has a new type of \u00f2value.\u00f3 we also show that potential games with strategy costs remain potential games. both zero-sum and potential games with strategy costs maintain a very appealing property: simple learning dynamics converge to nash equilibrium."}
{"paper": "nips84", "title": "learning annotated hierarchies from relational data", "abstract": "the objects in many real-world domains can be organized into hierarchies, where each internal node of a hierarchy picks out a category of objects. given a collection of features and relations de ned over a set of objects, an annotated hierarchy includes a speci cation of the categories that are most useful for describing each individual feature and relation. we de ne a generative model for annotated hierarchies and the features and relations that they describe, and develop a markov chain monte carlo scheme for learning annotated hierarchies. we show that our model discovers interpretable structure in several real-world data sets."}
{"paper": "nips85", "title": "the robustness-performance tradeoff in markov decision processes", "abstract": "computation of a satisfactory control policy for a markov decision process when the parameters of the model are uncertain is a problem encountered in many practical applications. the traditional robust approach is based on a worst-case analysis and may lead to an overly conservative policy. in this paper we consider the tradeoff between nominal performance and the worst case performance over all possible models. based on parametric linear programming, we propose a method that computes the whole set of pareto efficient policies in the performance-robustness plane when only the reward parameters are subject to uncertainty. in the more general case when the transition probabilities are also subject to error, we show that the strategy with the \u00f2optimal\u00f3 tradeoff might be non-markovian and hence is in general not tractable."}
{"paper": "nips86", "title": "bayesian detection of infrequent differences in sets of time series with shared structure", "abstract": "we present a hierarchical bayesian model for sets of related, but different, classes of time series data. our model performs alignment simultaneously across all classes, while detecting and characterizing class-specific differences. during inference the model produces, for each class, a distribution over a canonical representation of the class. these class-specific canonical representations are automatically aligned to one another \u00f1 preserving common sub-structures, and highlighting differences. we apply our model to compare and contrast solenoid valve current data, and also, liquid-chromatography-ultraviolet-diode array data from a study of the plant arabidopsis thaliana."}
{"paper": "nips87", "title": "sparse kernel orthonormalized pls for feature extraction in large data sets", "abstract": "in this paper we are presenting a novel multivariate analysis method. our scheme is based on a novel kernel orthonormalized partial least squares (pls) variant for feature extraction, imposing sparsity constrains in the solution to improve scalability. the algorithm is tested on a benchmark of uci data sets, and on the analysis of integrated short-time music features for genre prediction. the upshot is that the method has strong expressive power even with rather few features, is clearly outperforming the ordinary kernel pls, and therefore is an appealing method for feature extraction of labelled data."}
{"paper": "nips88", "title": "learnability and the doubling dimension", "abstract": "given a set a0 of classi ers and a probability distribution over their domain, one can de ne a metric by taking the distance between a pair of classi ers to be the probability that they classify a random item differently. we prove bounds on the sample complexity of pac learning in terms of the doubling dimension of this metric. these bounds imply known bounds on the sample complexity of learning halfspaces with respect to the uniform distribution that are optimal up to a constant factor. we prove a bound that holds for any algorithm that outputs a classi er with zero error whenever this is possible; this bound is in terms of the maximum of the doubling dimension and the vc-dimension of a0 , and strengthens the best known bound in terms of the vc-dimension alone. we show that there is no bound on the doubling dimension in terms of the vc-dimension of a0 (in contrast with the metric dimension)."}
{"paper": "nips89", "title": "mutagenetic tree fisher kernel improves prediction of hiv drug resistance from viral genotype", "abstract": "starting with the work of jaakkola and haussler, a variety of approaches have been proposed for coupling domain-speci c generative models with a wider range of statistical learning methods. the link is established by a kernel function which provides a measure of similarity based inherently on the underlying model. in computational biology, the full promise of this framework is rarely ever exploited, as most kernels are derived from very generic models, such as sequence pro les or hidden markov models. here, we introduce the mtreemix kernel, which is based on a generative model speci cally tailored to the underlying biological mechanism. in particular, it quanti es the similarity of evolutionary escape from antiviral drug pressure between two sequence samples. we compare this novel kernel to a standard, evolution-agnostic amino acid encoding in the prediction of hiv drug resistance from genotype, using support vector regression. the results show signi cant improvements in predictive performance across 17 anti-hiv drugs. thus, in our study, the generative-discriminative paradigm is key to bridging the gap between population genetic modeling and clinical decision making."}
{"paper": "nips90", "title": "non-rigid point set registration: coherent point drift", "abstract": "we introduce coherent point drift (cpd), a novel probabilistic method for nonrigid registration of point sets. the registration is treated as a maximum likelihood (ml) estimation problem with motion coherence constraint over the velocity eld such that one point set moves coherently to align with the second set. we formulate the motion coherence constraint and derive a solution of regularized ml estimation through the variational approach, which leads to an elegant kernel form. we also derive the em algorithm for the penalized ml optimization with deterministic annealing. the cpd method simultaneously nds both the non-rigid transformation and the correspondence between two point sets without making any prior assumption of the transformation model except that of motion coherence. this method can estimate complex non-linear non-rigid transformations, and is shown to be accurate on 2d and 3d examples and robust in the presence of outliers and missing points."}
{"paper": "nips91", "title": "towards a general independent subspace analysis", "abstract": "the increasingly popular independent component analysis (ica) may only be applied to data following the generative ica model in order to guarantee algorithmindependent and theoretically valid results. subspace ica models generalize the assumption of component independence to independence between groups of components. they are attractive candidates for dimensionality reduction methods, however are currently limited by the assumption of equal group sizes or less general semi-parametric models. by introducing the concept of irreducible independent subspaces or components, we present a generalization to a parameter-free mixture model. moreover, we relieve the condition of at-most-one-gaussian by including previous results on non-gaussian component analysis. after introducing this general model, we discuss joint block diagonalization with unknown block sizes, on which we base a simple extension of jade to algorithmically perform the subspace analysis. simulations confirm the feasibility of the algorithm."}
{"paper": "nips92", "title": "multiple instance learning for computer aided diagnosis", "abstract": "many computer aided diagnosis (cad) problems can be best modelled as a multiple-instance learning (mil) problem with unbalanced data: i.e. , the training data typically consists of a few positive bags, and a very large number of negative instances. existing mil algorithms are much too computationally expensive for these datasets. we describe ch, a framework for learning a convex hull representation of multiple instances that is significantly faster than existing mil algorithms. our ch framework applies to any standard hyperplane-based learning algorithm, and for some algorithms, is guaranteed to find the global optimal solution. experimental studies on two different cad applications further demonstrate that the proposed algorithm significantly improves diagnostic accuracy when compared to both mil and traditional classifiers. although not designed for standard mil problems (which have both positive and negative bags and relatively balanced datasets), comparisons against other mil methods on benchmark problems also indicate that the proposed method is competitive with the state-of-the-art."}
{"paper": "nips93", "title": "learning to model spatial dependency: semi-supervised discriminative random fields", "abstract": "we present a novel, semi-supervised approach to training discriminative random fields (drfs) that efficiently exploits labeled and unlabeled training data to achieve improved accuracy in a variety of image processing tasks. we formulate drf training as a form of map estimation that combines conditional loglikelihood on labeled data, given a data-dependent prior, with a conditional entropy regularizer defined on unlabeled data. although the training objective is no longer concave, we develop an efficient local optimization procedure that produces classifiers that are more accurate than ones based on standard supervised drf training. we then apply our semi-supervised approach to train drfs to segment both synthetic and real data sets, and demonstrate significant improvements over supervised drfs in each case."}
{"paper": "nips94", "title": "learning with hypergraphs: clustering, classification, and embedding", "abstract": "we usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. in many real-world problems, however, relationships among the objects of our interest are more complex than pairwise. naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information, which can be expected valuable for our learning tasks however. therefore, we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classi cation on the basis of the spectral hypergraph clustering approach. we have applied the hypergraph based approaches to real-world problems and the obtained results showed the advantages of hypergraphs over usual graphs."}
{"paper": "nips95", "title": "graph regularization for maximum variance unfolding with an application to sensor localization", "abstract": "theproblemofdiscoveringlowdimensionalrepresentationsarisesinsuchdiverse fields of engineering as robot navigation, protein clustering, shape recognition, and sensor localization. recently, researchers in all these areas have converged on common solutions using methods from convex optimization. in particular, many results have been obtained by constructing semidefinite programs (sdps) with low rank solutions. while the rank of matrix variables in sdps cannot be directly constrained, it has been observed that low rank solutions emerge naturally by computing high variance or maximal trace solutions that respect local distance constraints. in this paper, we show how to solve very large problems of this type by a matrix factorization that leads to much smaller sdps than those previously studied. the matrix factorization is derived by expanding the solution of the original problem in terms of the bottom eigenvectors of a graph laplacian. the smaller sdps obtained from this matrix factorization yield very good approximations to solutions of the original problem. moreover, these approximations can be further refined by conjugate gradient descent. we illustrate the approach on localization in large scale sensor networks, where optimizations involving tens of thousands of nodes can be solved in just a few minutes."}
{"paper": "nips96", "title": "a selective attention multi--chip system with dynamic synapses and spiking neurons", "abstract": "selective attention is the strategy used by biological sensory systems to solve the problem of limited parallel processing capacity: salient subregions of the input stimuli are serially processed, while non\u00f0salient regions are suppressed. we present an mixed mode analog/digital very large scale integration implementation of a building block for a multi\u00f0chip neuromorphic hardware model of selective attention. we describe the chip\u00f5s architecture and its behavior, when its is part of a multi\u00f0chip system with a spiking retina as input, and show how it can be used to implement in real-time flexible models of bottom-up attention."}
{"paper": "nips97", "title": "neurophysiological evidence of cooperative mechanisms for stereo computation", "abstract": "although there has been substantial progress in understanding the neurophysiological mechanisms of stereopsis, how neurons interact in a network during stereo computation remains unclear. computational models on stereopsis suggest local competition and long-range cooperation are important for resolving ambiguity during stereo matching. to test these predictions, we simultaneously recorded from multiple neurons in v1 of awake, behaving macaques while presenting surfaces of different depths rendered in dynamic random dot stereograms. we found that the interaction between pairs of neurons was a function of similarity in receptive fields, as well as of the input stimulus. neurons coding the same depth experienced common inhibition early in their responses for stimuli presented at their nonpreferred disparities. they experienced mutual facilitation later in their responses for stimulation at their preferred disparity. these findings are consistent with a local competition mechanism that first removes gross mismatches, and a global cooperative mechanism that further refines depth estimates."}
{"paper": "nips98", "title": "hierarchical dirichlet processes with random effects", "abstract": "data sets involving multiple groups with shared characteristics frequently arise in practice. in this paper we extend hierarchical dirichlet processes to model such data. each group is assumed to be generated from a template mixture model with group level variability in both the mixing proportions and the component parameters. variabilities in mixing proportions across groups are handled using hierarchical dirichlet processes, also allowing for automatic determination of the number of components. in addition, each group is allowed to have its own component parameters coming from a prior described by a template mixture model. this group-level variability in the component parameters is handled using a random effects model. we present a markov chain monte carlo (mcmc) sampling algorithm to estimate model parameters and demonstrate the method by applying it to the problem of modeling spatial brain activation patterns across multiple images collected via functional magnetic resonance imaging (fmri)."}
{"paper": "nips99", "title": "context dependent amplification of both rate and event-correlation in a vlsi network of spiking neurons", "abstract": "cooperative competitive networks are believed to play a central role in cortical processing and have been shown to exhibit a wide set of useful computational properties. we propose a vlsi implementation of a spiking cooperative competitive network and show how it can perform context dependent computation both in the mean firing rate domain and in spike timing correlation space. in the mean rate case the network amplifies the activity of neurons belonging to the selected stimulus and suppresses the activity of neurons receiving weaker stimuli. in the event correlation case, the recurrent network amplifies with a higher gain the correlation between neurons which receive highly correlated inputs while leaving the mean firing rate unaltered. we describe the network architecture and present experimental data demonstrating its context dependent computation capabilities."}
{"paper": "nips100", "title": "stability of $k$-means clustering", "abstract": "we phrase k-means clustering as an empirical risk minimization procedure over a class hk and explicitly calculate the covering number for this class. next, we show that stability of k-means clustering is characterized by the geometry of hk with respect to the underlying distribution. we prove that in the case of a unique global minimizer, the clustering solution is stable with respect to complete changes of the data, while for the case of multiple minimizers, the change of \u00fd(n1=2) samples defines the transition between stability and instability. while for a finite number of minimizers this result follows from multinomial distribution estimates, the case of infinite minimizers requires more refined tools. we conclude by proving that stability of the functions in hk implies stability of the actual centers of the clusters. since stability is often used for selecting the number of clusters in practice, we hope that our analysis serves as a starting point for finding theoretically grounded recipes for the choice of k."}
{"paper": "nips101", "title": "learning on graph with laplacian regularization", "abstract": "we consider a general form of transductive learning on graphs with laplacian regularization, and derive margin-based generalization bounds using appropriate geometric properties of the graph. we use this analysis to obtain a better understanding of the role of normalization of the graph laplacian matrix as well as the effect of dimension reduction. the results suggest a limitation of the standard degree-based normalization. we propose a remedy from our analysis and demonstrate empirically that the remedy leads to improved classification performance."}
{"paper": "nips102", "title": "learning time-intensity profiles of human activity using non-parametric bayesian models", "abstract": "data sets that characterize human activity over time through collections of timestamped events or counts are of increasing interest in application areas as humancomputer interaction, video surveillance, and web data analysis. we propose a non-parametric bayesian framework for modeling collections of such data. in particular, we use a dirichlet process framework for learning a set of intensity functions corresponding to different categories, which form a basis set for representing individual time-periods (e.g., several days) depending on which categories the time-periods are assigned to. this allows the model to learn in a data-driven fashion what \u00f2factors\u00f3 are generating the observations on a particular day, including (for example) weekday versus weekend effects or day-specific effects corresponding to unique (single-day) occurrences of unusual behavior, sharing information where appropriate to obtain improved estimates of the behavior associated with each category. applications to real\u00f0world data sets of count data involving both vehicles and people are used to illustrate the technique."}
{"paper": "nips103", "title": "relational learning with gaussian processes", "abstract": "correlation between instances is often modelled via a kernel function using input attributes of the instances. relational knowledge can further reveal additional pairwise correlations between variables of interest. in this paper, we develop a class of models which incorporates both reciprocal relational information and input attributes using gaussian process techniques. this approach provides a novel non-parametric bayesian framework with a data-dependent covariance function for supervised learning tasks. we also apply this framework to semi-supervised learning. experimental results on several real world data sets verify the usefulness of this algorithm."}
{"paper": "nips104", "title": "shifting, one-inclusion mistake bounds and tight multiclass expected risk bounds", "abstract": "under the prediction model of learning, a prediction strategy is presented with an i.i.d. sample of n 1 points in x and corresponding labels from a concept f2f, and aims to minimize the worst-case probability of erring on an nth point. by exploiting the structure of f, haussler et al. achieved a vc(f)=n bound for the natural one-inclusion prediction strategy, improving on bounds implied by pac-type results by a o(logn) factor. the key data structure in their result is the natural subgraph of the hypercube\u00f1the one-inclusion graph; the key step is a d = vc(f) bound on one-inclusion graph density. the first main result of this paper is a density bound of n n 1 d 1 =( n d ) < d, which positively resolves a conjecture of kuzmin & warmuth relating to their unlabeled peeling compression scheme and also leads to an improved mistake bound for the randomized (deterministic) one-inclusion strategy for all d (for d (n)). the proof uses a new form of vc-invariant shifting and a group-theoretic symmetrization. our second main result is a k-class analogue of the d=n mistake bound, replacing the vc-dimension by the pollard pseudo-dimension and the one-inclusion strategy by its natural hypergraph generalization. this bound on expected risk improves on known pac-based results by a factor ofo(logn) and is shown to be optimal up to a o(logk) factor. the combinatorial technique of shifting takes a central role in understanding the one-inclusion (hyper)graph and is a running theme throughout."}
{"paper": "nips105", "title": "linearly-solvable markov decision problems", "abstract": "to appear in advances in neural information processing systems 2006 we introduce a class of markov decision problems (mdps) which greatly simplify reinforcement learning. these mdps have discrete state spaces and continuous control spaces. the controls have the effect of scaling the transition probabilities of an underlying markov chain. a control cost penalizing kl divergence between controlled and uncontrolled transition probabilities makes the minimization problem convex and allows analytical computation of the optimal controls given the optimal value function. an exponential transformation of the optimal value function makes the minimized bellman equation linear. apart from their theoretical signi\u0002cance, the new mdps enable ef\u0002cient approximations to traditional mdps. shortest path problems are approximated to arbitrary precision with largest eigenvalue problems, yielding an o(n) algorithm. accurate approximations to generic mdps are obtained via continuous embedding. off-policy learning of the optimal value function is possible without need for state-action values; the new algorithm (z-learning) outperforms q-learning. this work was supported by nsf grant ecs\u00960524761."}
{"paper": "nips106", "title": "part-based probabilistic point matching using equivalence constraints", "abstract": "correspondence algorithms typically struggle with shapes that display part-based variation. we present a probabilistic approach that matches shapes using independent part transformations, where the parts themselves are learnt during matching. ideas from semi-supervised learning are used to bias the algorithm towards finding \u00f4perceptually valid\u00f5 part structures. shapes are represented by unlabeled point sets of arbitrary size and a background component is used to handle occlusion, local dissimilarity and clutter. thus, unlike many shape matching techniques, our approach can be applied to shapes extracted from real images. model parameters are estimated using an em algorithm that alternates between finding a soft correspondence and computing the optimal part transformations using procrustes analysis."}
{"paper": "nips107", "title": "modeling human motion using binary latent variables", "abstract": "we propose a non-linear generative model for human motion data that uses binary latent variables which are symmetrically connected to real-valued \u00f2visible\u00f3 variables that represent joint angles. the latent variables at each time step receive directed connections from the latent variables at one or two previous time-steps. the undirected observation model makes online inference efficient and allows us to use a simple approximate learning procedure. trained in this way, our architecture can find a single set of parameters that captures several different kinds of motion. we demonstrate various synthesized motion sequences and show that data lost during the motion capture process can be filled in online."}
{"paper": "nips108", "title": "detecting humans via their pose", "abstract": "we consider the problem of detecting humans and classifying their pose from a single image. specifically, our goal is to devise a statistical model that simultaneously answers two questions: 1) is there a human in the image? and, if so, 2) what is a low-dimensional representation of her pose? we investigate models that can be learned in an unsupervised manner on unlabeled images of human poses, and provide information that can be used to match the pose of a new image to the ones present in the training set. starting from a set of descriptors recently proposed for human detection, we apply the latent dirichlet allocation framework to model the statistics of these features, and use the resulting model to answer the above questions. we show how our model can efficiently describe the space of images of humans with their pose, by providing an effective representation of poses for tasks such as classification and matching, while performing remarkably well in human/non human decision problems, thus enabling its use for human detection. we validate the model with extensive quantitative experiments and comparisons with other approaches on human detection and pose matching."}
{"paper": "nips109", "title": "clustering under prior knowledge with application to image segmentation", "abstract": "this paper proposes a new approach to model-based clustering under prior knowledge. the proposed formulation can be interpreted from two different angles: as penalized logistic regression, where the class labels are only indirectly observed (via the probability density of each class); as finite mixture learning under a grouping prior. to estimate the parameters of the proposed model, we derive a (generalized) em algorithm with a closed-form e-step, in contrast with other recent approaches to semi-supervised probabilistic clustering which require gibbs sampling or suboptimal shortcuts. we show that our approach is ideally suited for image segmentation: it avoids the combinatorial nature markov random field priors, and opens the door to more sophisticated spatial priors (e.g., wavelet-based) in a simple and computationally efficient way. finally, we extend our formulation to work in unsupervised, semi-supervised, or discriminative modes."}
{"paper": "nips110", "title": "cross-validation optimization for large scale hierarchical classification kernel methods", "abstract": "we propose a highly efficient framework for kernel multi-class models with a large and structured set of classes. kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. we demonstrate our approach on large scale text classification tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work."}
{"paper": "nips111", "title": "boosting structured prediction for imitation learning", "abstract": "the maximum margin planning (mmp) (ratliff et al., 2006) algorithm solves imitation learning problems by learning linear mappings from features to cost functions in a planning domain. the learned policy is the result of minimum-cost planning using these cost functions. these mappings are chosen so that example policies (or trajectories) given by a teacher appear to be lower cost (with a lossscaled margin) than any other policy for a given planning domain. we provide a novel approach, mmpboost , based on the functional gradient descent view of boosting (mason et al., 1999; friedman, 1999a) that extends mmp by \u00f2boosting\u00f3 in new features. this approach uses simple binary classification or regression to improve performance of mmp imitation learning, and naturally extends to the class of structured maximum margin prediction problems. (taskar et al., 2005) our technique is applied to navigation and planning problems for outdoor mobile robots and robotic legged locomotion."}
{"paper": "nips112", "title": "map-reduce for machine learning on multicore", "abstract": "we are at the beginning of the multicore era. computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. in this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. specifically, we show that algorithms that fit the statistical query model [15] can be written in a certain \u00f2summation form,\u00f3 which allows them to be easily parallelized on multicore computers. we adapt google\u00f5s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (lwlr), k-means, logistic regression (lr), naive bayes (nb), svm, ica, pca, gaussian discriminant analysis (gda), em, and backpropagation (nn). our experimental results show basically linear speedup with an increasing number of processors."}
{"paper": "nips113", "title": "game theoretic algorithms for protein-dna binding", "abstract": "we develop and analyze game-theoretic algorithms for predicting coordinate binding of multiple dna binding regulators. the allocation of proteins to local neighborhoods and to sites is carried out with resource constraints while explicating competing and coordinate binding relations among proteins with affinity to the site or region. the focus of this paper is on mathematical foundations of the approach. we also briefly demonstrate the approach in the context of the ?-phage switch."}
{"paper": "nips114", "title": "greedy layer-wise training of deep networks", "abstract": "deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. however, until recently it was not clear how to train such deep networks, since gradientbased optimization starting from random initialization appears to often get stuck in poor solutions. hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for deep belief networks (dbn), a generative model with many layers of hidden causal variables. in the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input bringing better generalization."}
{"paper": "nips115", "title": "randomized clustering forests for building fast and discriminative visual vocabularies", "abstract": "some of the most effective recent methods for content-based image classification work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting \u00f2visual word\u00f3 codes over the image, and classifying these with a conventional classifier such as an svm. large numbers of descriptors and largecodebooksareneededforgoodresultsandthisbecomesslowusingk-means. we introduce extremely randomized clustering forests \u00f0 ensembles of randomly created clustering trees \u00f0 and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classification tasks."}
{"paper": "nips116", "title": "online clustering of moving hyperplanes", "abstract": "we propose a recursive algorithm for clustering trajectories lying in multiple moving hyperplanes. starting from a given or random initial condition, we use normalized gradient descent to update the coefficients of a time varying polynomial whose degree is the number of hyperplanes and whose derivatives at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory. as time proceeds, the estimates of the hyperplane normals are shown to track their true values in a stable fashion. the segmentation of the trajectories is then obtained by clustering their associated normal vectors. the final result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes. we test our algorithm on the segmentation of dynamic scenes containing rigid motions and dynamic textures, e.g., a bird floating on water. our method not only segments the bird motion from the surrounding water motion, but also determines patterns of motion in the scene (e.g., periodic motion) directly from the temporal evolution of the estimated polynomial coefficients. our experiments also show that our method can deal with appearing and disappearing motions in the scene."}
{"paper": "nips117", "title": "an information theoretic framework for eukaryotic gradient sensing", "abstract": "chemical reaction networks by which individual cells gather and process information about their chemical environments have been dubbed \u00f2signal transduction\u00f3 networks. despite this suggestive terminology, there have been few attempts to analyze chemical signaling systems with the quantitative tools of information theory. gradient sensing in the social amoeba dictyostelium discoideum is a well characterized signal transduction system in which a cell estimates the direction of a source of diffusing chemoattractant molecules based on the spatiotemporal sequence of ligand-receptor binding events at the cell membrane. using monte carlo techniques (mcell) we construct a simulation in which a collection of individual ligand particles undergoing brownian diffusion in a three-dimensional volume interact with receptors on the surface of a static amoeboid cell. adapting amethodforestimationofspiketrainentropiesdescribedbyvictor(originallydue to kozachenko and leonenko), we estimate lower bounds on the mutual information between the transmitted signal (direction of ligand source) and the received signal (spatiotemporal pattern of receptor binding/unbinding events). hence we provideaquantitativeframeworkforaddressingthequestion: howmuchcouldthe cell know, and when could it know it? we show that the time course of the mutual information between the cell\u00f5s surface receptors and the (unknown) gradient direction is consistent with experimentally measured cellular response times. we find that the acquisition of directional information depends strongly on the time constant at which the intracellular response is filtered."}
{"paper": "nips118", "title": "learning to traverse image manifolds", "abstract": "we present a new algorithm, locally smooth manifold learning (lsml), that learns a warping function from a point on an image manifold to its neighbors. important characteristics of lsml include the ability to recover the structure of the manifold in sparsely populated regions and beyond the support of the provided data. applications of our proposed technique include embedding with a natural out-of-sample extension and tasks such as tangent distance estimation, frame rate up-conversion, video compression and motion transfer."}
{"paper": "nips119", "title": "sparse representation for signal classification", "abstract": "in this paper, application of sparse representation (factorization) of signals over an overcomplete basis (dictionary) for signal classification is discussed. searching for the sparse representation of a signal over an overcomplete dictionary is achieved by optimizing an objective function that includes two terms: one that measures the signal reconstruction error and another that measures the sparsity. this objective function works well in applications where signals need to be reconstructed, like coding and denoising. on the other hand, discriminative methods, such as linear discriminative analysis (lda), are better suited for classification tasks. however, discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruction. in this paper, we present a theoretical framework for signal classification with sparse representation. the approach combines the discrimination power of the discriminative methods with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruptions: noise, missing data and outliers. the proposed approach is therefore capable of robust classification with a sparse representation of signals. the theoretical results are demonstrated with signal classification tasks, showing that the proposed approach outperforms the standard discriminative methods and the standard sparse representation in the case of corrupted signals."}
{"paper": "nips120", "title": "image retrieval and classification using local distance functions", "abstract": "in this paper we introduce and experiment with a framework for learning local perceptual distance functions for visual recognition. we learn a distance function for each individual training image as a combination of elementary distances between visual features. we apply these combined local distance functions to the tasks of image retrieval and classification of novel images. on the caltech 101 object recognition benchmark, we achieve 59% recognition using 15 training images, which matches the best published performance by zhang, et al."}
{"paper": "nips121", "title": "a theory of retinal population coding", "abstract": "efficient coding models predict that the optimal code for natural images is a population of oriented gabor receptive fields. these results match response properties in primary visual cortex, but not those in the retina. does the retina use an optimal code, and if so, what is it optimized for? one issue that has not been incorporated into theoretical models is that the image sampled by retinal photoreceptors is degraded both by the optics of the eye and by the photon transduction noise. therefore, de-blurring and de-noising of the retinal signal should be an important aspect of retinal coding. furthermore, this must be implemented under biologically plausible conditions such as limited neural precision and population size. here we present an optimal encoding method that simultaneously achieves deblurring, de-noising, and maximal preservation of undistorted image signal using a noisy neural population. filters optimized for natural images show strong similarities to retinal ganglion cell (rgc) receptive fields. importantly, the model predicts different receptive field populations with retinal eccentricities where the optical blur and the number of rgcs are significantly different. the proposed model provides a unified account of the retinal coding, and it may also be viewed as a generalization of the wiener filter with an arbitrary number of noisy units."}
{"paper": "nips122", "title": "kernels on structured objects through nested histograms", "abstract": "we propose a family of kernels for structured objects which is based on the bag-ofcomponents paradigm. however, rather than decomposing each complex object into the single histogram of its components, we use for each object a family of nested histograms, where each histogram in this hierarchy describes the object seen from an increasingly granular perspective. we use this hierarchy of histograms to define elementary kernels which can detect coarse and fine similarities between the objects. we compute through an efficient averaging trick a mixture of such specific kernels, to propose a final kernel value which weights efficiently local and global matches. we propose experimental results on an image retrieval experiment which show that this mixture is an effective template procedure to be used with kernels on histograms."}
{"paper": "nips123", "title": "prediction on a graph with a perceptron", "abstract": "we study the problem of online prediction of a noisy labeling of a graph with the perceptron. we address both label noise and concept noise. graph learning is framedasaninstanceofpredictiononafiniteset. totreatlabelnoiseweshowthat the hinge loss bounds derived by gentile [1] for online perceptron learning can be transformed to relative mistake bounds with an optimal leading constant when applied to prediction on a finite set. these bounds depend crucially on the norm of the learned concept. often the norm of a concept can vary dramatically with only small perturbations in a labeling. we analyze a simple transformation that stabilizes the norm under perturbations. we derive an upper bound that depends only on natural properties of the graph \u00f0 the graph diameter and the cut size of a partitioning of the graph \u00f0 which are only indirectly dependent on the size of the graph. the impossibility of such bounds for the graph geodesic nearest neighbors algorithm will be demonstrated."}
{"paper": "nips124", "title": "on the relation between low density separation, spectral clustering and graph cuts", "abstract": "one of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. in this paper we provide some formal analysis of that notion for a probability distribution. we introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. we show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary. keywords: clustering, semi-supervised learning"}
{"paper": "nips125", "title": "large margin component analysis", "abstract": "metric learning has been shown to significantly improve the accuracy of k-nearest neighbor (knn) classification. in problems involving thousands of features, distance learning algorithms cannot be used due to overfitting and high computational complexity. in such cases, previous work has relied on a two-step solution: first apply dimensionality reduction methods to the data, and then learn a metric in the resulting low-dimensional subspace. in this paper we show that better classification performance can be achieved by unifying the objectives of dimensionality reduction and metric learning. we propose a method that solves for the low-dimensional projection of the inputs, which minimizes a metric objective aimed at separating points in different classes by a large margin. this projection is defined by a significantly smaller number of parameters than metrics learned in input space, and thus our optimization reduces the risks of overfitting. theory and results are presented for both a linear as well as a kernelized version of the algorithm. overall, we achieve classification rates similar, and in several cases superior, to those of support vector machines."}
{"paper": "nips126", "title": "efficient learning of sparse representations with an energy-based model", "abstract": "we describe a novel unsupervised method for learning sparse, overcomplete features. the model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. learning proceeds in a two-phase em-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. the model produces \u00f2stroke detectors\u00f3 when trained on handwritten numerals, and gabor-like filters when trained on natural image patches. inference and learning are very fast, requiring no preprocessing, and no expensive sampling. using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the mnist dataset. finally, an extension of the method is described to learn topographical filter maps."}
{"paper": "nips127", "title": "unsupervised learning of a probabilistic grammar for object detection and parsing", "abstract": "we describe an unsupervised method for learning a probabilistic grammar of an object from a set of training examples. our approach is invariant to the scale and rotation of the objects. we illustrate our approach using thirteen objects from the caltech 101 database. in addition, we learn the model of a hybrid object class where we do not know the specific object or its position, scale or pose. this is illustrated by learning a hybrid class consisting of faces, motorbikes, and airplanes. the individual objects can be recovered as different aspects of the grammar for the object class. in all cases, we validate our results by learning the probability grammars from training datasets and evaluating them on the test datasets. we compare our method to alternative approaches. the advantages of our approach is the speed of inference (under one second), the parsing of the object, and increased accuracy of performance. moreover, our approach is very general and can be applied to a large range of objects and structures."}
{"paper": "nips128", "title": "fast iterative kernel pca", "abstract": "we introduce two methods to improve convergence of the kernel hebbian algorithm (kha) for iterative kernel pca. kha has a scalar gain parameter which is either held constant or decreased as 1/t, leading to slow convergence. our kha/et algorithm accelerates kha by incorporating the reciprocal of the current estimated eigenvalues as a gain vector. we then derive and apply stochastic meta- descent (smd) to kha/et; this further speeds convergence by performing gain adaptation in rkhs. experimental results for kernel pca and spectral clustering of usps digits as well as motion capture and image de-noising problems confirm that our methods converge substantially faster than conventional kha."}
{"paper": "nips129", "title": "structure learning in markov random fields", "abstract": "scoring structures of undirected graphical models by means of evaluating the marginal likelihood is very hard. the main reason is the presence of the partition function which is intractable to evaluate, let alone integrate over. we propose to approximate the marginal likelihood by employing two levels of approximation: we assume normality of the posterior (the laplace approximation) and approximate all remaining intractable quantities using belief propagation and the linear response approximation. this results in a fast procedure for model scoring. empirically, we find that our procedure has about two orders of magnitude better accuracy than standard bic methods for small datasets, but deteriorates when the size of the dataset grows."}
{"paper": "nips130", "title": "ilstd: eligibility traces and convergence analysis", "abstract": "we present new theoretical and empirical results with the ilstd algorithm for policy evaluation in reinforcement learning with linear function approximation. ilstd is an incremental method for achieving results similar to lstd, the dataefficient, least-squares version of temporal difference learning, without incurring the full cost of the lstd computation. lstd is o(n2), where n is the number of parameters in the linear function approximator, while ilstd is o(n). in this paper, we generalize the previous ilstd algorithm and present three new results: (1) the first convergence proof for an ilstd algorithm; (2) an extension to incorporate eligibility traces without changing the asymptotic computational complexity; and (3) the first empirical results with an ilstd algorithm for a problem (mountain car) with feature vectors large enough (n = 10,000) to show substantial computational advantages over lstd."}
{"paper": "nips131", "title": "fast computation of graph kernels", "abstract": "using extensions of linear algebra concepts to reproducing kernel hilbert spaces (rkhs), we define a unifying framework for random walk kernels on graphs. reduction to a sylvester equation allows us to compute many of these kernels in o(n3) worst-case time. this includes kernels whose previous worst-case time complexity was o(n6), such as the geometric kernels of g\u00acartner et al. [1] and the marginal graph kernels of kashima et al. [2]. our algebra in rkhs allow us to exploit sparsity in directed and undirected graphs more effectively than previous methods, yielding sub-cubic computational complexity when combined with conjugate gradient solvers or fixed-point iterations. experiments on graphs from bioinformatics and other application domains show that our algorithms are often more than 1000 times faster than existing approaches."}
{"paper": "nips132", "title": "analysis of representations for domain adaptation", "abstract": "discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. in many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. under what conditions can we adapt a classifier trained on the source domain for use in the target domain? intuitively, a good feature representation is a crucial factor in the success of domain adaptation. we formalize this intuition theoretically with a generalization bound for domain adaption. our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model. it also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set."}
{"paper": "nips133", "title": "automated hierarchy discovery for planning in partially observable domains", "abstract": "planning in partially observable domains is a notoriously difficult problem. however, in many real-world scenarios, planning can be simplified by decomposing the task into a hierarchy of smaller planning problems. several approaches have been proposed to optimize a policy that decomposes according to a hierarchy specified a priori. in this paper, we investigate the problem of automatically discovering the hierarchy. more precisely, we frame the optimization of a hierarchical policy as a non-convex optimization problem that can be solved with general non-linear solvers, a mixed-integer linear approximation or a form of bounded hierarchical policy iteration. by encoding the hierarchical structure as variables of the optimization problem, we can automatically discover a hierarchy. our method is flexible enough to allow any parts of the hierarchy to be specified based on prior knowledge while letting the optimization discover the unknown parts. it can also discover hierarchical policies, including recursive policies, that are more compact (potentially infinitely fewer parameters) and often easier to understand given the decomposition induced by the hierarchy."}
{"paper": "nips134", "title": "conditional random sampling: a sketch-based sampling technique for sparse data", "abstract": "we1 develop conditional random sampling (crs), a technique particularly suitable for sparse data. in large-scale applications, the data are often highly sparse. crs combines sketching and sampling in that it converts sketches of the data into conditional random samples online in the estimation stage, with the sample size determined retrospectively. this paper focuses on approximating pairwise l2 and l1 distances and comparing crs with random projections. for boolean (0/1) data, crs is provably better than random projections. we show using real-world data that crs often outperforms random projections. this technique can be applied in learning, data mining, information retrieval, and database query optimizations."}
{"paper": "nips135", "title": "efficient structure learning of markov networks using l1-regularization", "abstract": "markov networks are widely used in a wide variety of applications, in problems ranging from computer vision, to natural language, to computational biology. in most current applications, even those that rely heavily on learned models, the structure of the markov network is constructed by hand, due to the lack of effective algorithms for learning markov network structure from data. in this paper, we provide a computationally effective method for learning markov network structure from data. our method is based on the use of l1 regularization on the weights of the log-linear model, which has the effect of biasing the model towards solutions where many of the parameters are zero. this formulation converts the markov network learning problem into a convex optimization problem in a continuous space, which can be solved using efficient gradient methods. a key issue in this setting is the (unavoidable) use of approximate inference, which can lead to errors in the gradient computation when the network structure is dense. thus, we explore the use of different feature introduction schemes and compare their performance. we provide results for our method on synthetic data, and on two real world data sets: modeling the joint distribution of pixel values in the mnist data, and modeling the joint distribution of genetic sequence variations in the human hapmap data. we show that our l1-based method achieves considerably higher generalization performance than the more standard l2-based method (a gaussian parameter prior) or pure maximum-likelihood learning. we also show that we can learn mrf network structure at a computational cost that is not much greater than learning parameters alone, demonstrating the existence of a feasible method for this important problem."}
{"paper": "nips136", "title": "bayesian policy gradient algorithms", "abstract": "policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. conventional policy gradient methods use monte-carlo techniques to estimate this gradient, and improve the policy by adjusting the parameters in the direction of that gradient estimate. since monte carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. in this paper, we propose a bayesian framework that models the policy gradient as a gaussian process. this reduces the number of samples needed to obtain accurate gradient estimates. moreover, estimates of the natural gradient and a measure of the uncertainty in the gradient estimates are provided at little extra cost."}
{"paper": "nips137", "title": "in-network pca and anomaly detection", "abstract": "we consider the problem of network anomaly detection in large distributed systems. in this setting, principal component analysis (pca) has been proposed as a method for discovering anomalies by continuously tracking the projection of the data onto a residual subspace. while successful empirically in small networks, this approach has serious scalability limitations. to overcome these limitations, we develop a pca-based anomaly detector in which adaptive local data filters send to a coordinator just enough data to enable accurate global detection. our method is based on a stochastic matrix perturbation analysis that characterizes the tradeoff between the accuracy of anomaly detection and the amount of data communicated over the network."}
{"paper": "nips138", "title": "pac-bayes bounds for the risk of the majority vote and the variance of the gibbs classifier", "abstract": "we propose new pac-bayes bounds for the risk of the weighted majority vote that depend on the mean and variance of the error of its associated gibbs classifier. we show that these bounds can be smaller than the risk of the gibbs classifier and can be arbitrarily close to zero even if the risk of the gibbs classifier is close to 1/2. moreover, we show that these bounds can be uniformly estimated on the training data for all possible posteriors q. moreover, they can be improved by using a large sample of unlabelled data."}
{"paper": "nips139", "title": "scalable discriminative learning for natural language parsing and translation", "abstract": "parsing and translating natural languages can be viewed as problems of predicting tree structures. for machine learning approaches to these predictions, the diversity and high dimensionality of the structures involved mandate very large training sets. this paper presents a purely discriminative learning method that scales up well to problems of this size. its accuracy was at least as good as other comparable methods on a standard parsing task. to our knowledge, it is the first purely discriminative learning algorithm for translation with treestructured models. unlike other popular methods, this method does not require a great deal of feature engineering a priori, because it performs feature selection over a compound feature space as it learns. experiments demonstrate the method\u00f5s versatility, accuracy, and efficiency. relevant software is freely available at http://nlp.cs.nyu.edu/parser and http://nlp.cs.nyu.edu/genpar."}
{"paper": "nips140", "title": "ordinal regression by extended binary classification", "abstract": "we present a reduction framework from ordinal regression to binary classification based on extended examples. the framework consists of three steps: extracting extended examples from the original examples, learning a binary classifier on the extended examples with any binary classification algorithm, and constructing a ranking rule from the binary classifier. a weighted 0/1 loss of the binary classifier would then bound the mislabeling cost of the ranking rule. our framework allows not only to design good ordinal regression algorithms based on well-tuned binary classification approaches, but also to derive new generalization bounds for ordinal regression from known bounds for binary classification. in addition, our framework unifies many existing ordinal regression algorithms, such as perceptron ranking and support vector ordinal regression. when compared empirically on benchmark data sets, some of our newly designed algorithms enjoy advantages in terms of both training speed and generalization performance over existing algorithms, which demonstrates the usefulness of our framework."}
{"paper": "nips141", "title": "training conditional random fields for maximum parse accuracy", "abstract": "we consider the problem of training a conditional random eld (crf) to maximize per-label predictive accuracy on a training set, an approach motivated by the principle of empirical risk minimization. we give a gradient-based procedure for minimizing an arbitrarily accurate approximation of the empirical risk under a hamming loss function. we present results which show that this optimization procedure can lead to signi cantly better testing performance than two other objective functions for crf training."}
{"paper": "nips142", "title": "graph-based visual saliency", "abstract": "a new bottom-up visual saliency model, graph-based visual saliency (gbvs), is proposed. it consists of two steps: \u0002rst forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. the model is simple, and biologically plausible insofar as it is naturally parallelized. this model powerfully predicts human \u0002xations on 749 variations of 108 natural images, achieving 98% of the roc area of a human-based control, whereas the classical algorithms of itti & koch ([2], [3], [4]) achieve only 84%."}
{"paper": "nips143", "title": "learning to parse images of articulated bodies", "abstract": "we consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. this problem is hard because of the large number of degrees of freedom to be estimated. following a established line of research, pose estimation is framed as inference in a probabilistic model. in our experience however, the success of many approaches often lie in the power of the features. our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. we show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the weizmann database."}
{"paper": "nips144", "title": "using combinatorial optimization within max-product belief propagation", "abstract": "in general, the problem of computing a maximum a posteriori (map) assignment in a markov random eld (mrf) is computationally intractable. however, in certain subclasses of mrf, an optimal or close-to-optimal assignment can be found very ef ciently using combinatorial optimization algorithms: certain mrfs with mutual exclusion constraints can be solved using bipartite matching, and mrfs with regular potentials can be solved using minimum cut methods. however, these solutions do not apply to the many mrfs that contain such tractable components as sub-networks, but also other non-complying potentials. in this paper, we present a new method, called compose, for exploiting combinatorial optimization for sub-networks within the context of a max-product belief propagation algorithm. compose uses combinatorial optimization for computing exact maxmarginals for an entire sub-network; these can then be used for inference in the context of the network as a whole. we describe highly ef cient methods for computing max-marginals for subnetworks corresponding both to bipartite matchings and to regular networks. we present results on both synthetic and real networks encoding correspondence problems between images, which involve both matching constraints and pairwise geometric constraints. we compare to a range of current methods, showing that the ability of compose to transmit information globally across the network leads to improved convergence, decreased running time, and higher-scoring assignments."}
{"paper": "nips145", "title": "clustering appearance and shape by learning jigsaws", "abstract": "patch-based appearance models are used in a wide range of computer vision applications. to learn such models it has previously been necessary to specify a suitable set of patch sizes and shapes by hand. in the jigsaw model presented here, the shape, size and appearance of patches are learned automatically from the repeated structures in a set of training images. by learning such irregularly shaped \u00f4jigsaw pieces\u00f5, we are able to discover both the shape and the appearance of object parts without supervision. when applied to face images, for example, the learned jigsaw pieces are surprisingly strongly associated with face parts of different shapes and scales such as eyes, noses, eyebrows and cheeks, to name a few. we conclude that learning the shape of the patch not only improves the accuracy of appearance-based part detection but also allows for shape-based part detection. this enables parts of similar appearance but different shapes to be distinguished; for example, while foreheads and cheeks are both skin colored, they have markedly different shapes."}
{"paper": "nips146", "title": "optimal change-detection and spiking neurons", "abstract": "survival in a non-stationary, potentially adversarial environment requires animals to detect sensory changes rapidly yet accurately, two oft competing desiderata. neurons subserving such detections are faced with the corresponding challenge to discern \u00f2real\u00f3 changes in inputs as quickly as possible, while ignoring noisy fluctuations. mathematically, this is an example of a change-detection problem that is actively researched in the controlled stochastic processes community. in this paper, we utilize sophisticated tools developed in that community to formalize an instantiation of the problem faced by the nervous system, and characterize the bayes-optimal decision policy under certain assumptions. we will derive from this optimal strategy an information accumulation and decision process that remarkably resembles the dynamics of a leaky integrate-and-fire neuron. this correspondence suggests that neurons are optimized for tracking input changes, and sheds new light on the computational import of intracellular properties such as resting membrane potential, voltage-dependent conductance, and post-spike reset voltage. we also explore the influence that factors such as timing, uncertainty, neuromodulation, and reward should and do have on neuronal dynamics and sensitivity, as the optimal decision strategy depends critically on these factors."}
{"paper": "nips147", "title": "correcting sample selection bias by unlabeled data", "abstract": "we consider the scenario where training and test data are drawn from different distributions, commonly referred to as sample selection bias. most algorithms for this setting try to rst recover sampling distributions and then make appropriate corrections based on the distribution estimate. we present a nonparametric method which directly produces resampling weights without distribution estimation. our method works by matching distributions between training and testing sets in feature space. experimental results demonstrate that our method works well in practice. note: preproceedings only; final version may differ."}
